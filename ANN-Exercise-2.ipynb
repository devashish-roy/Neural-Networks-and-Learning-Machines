{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to NNLM Exercise ANN2\n",
    "\n",
    "This exercise is split into three parts:\n",
    "- In part 1, you will learn the fundamentals of RNNs\n",
    "- In part 2, you will implement a basic RNN yourselves using numpy\n",
    "- In part 3, you will implement an LSTM using pytorch\n",
    "\n",
    "In this exercise you'll explore recurrent neural networks (RNNs), and some basic natural language processing (NLP). More specifically, your task will be to design an RNN from scratch using only numpy, similar to task 3 in ANN1. The goal of this network is to predict the correct next character given a sequence of characters. This is a simplified version of [causal language modelling](https://huggingface.co/docs/transformers/tasks/language_modeling), which is the main objective that is used to train large language models such as the [GPT series](http://jalammar.github.io/illustrated-gpt2/).\n",
    "\n",
    "## Reading material\n",
    "Before doing this exercise, please study the ANN2 module in Canvas and read Chapter 10 in the Deep Learning book, especially 10.2.2. If you want to learn more details about RNNs, we recommend [Andrej Karpathy's blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [this lecture](https://www.youtube.com/watch?v=0LixFSa7yts) and [this lecture](https://www.youtube.com/watch?v=6niqTuYFZLQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - RNN theory\n",
    "## What is an RNN?\n",
    "An RNN is a neural network just like the network you designed in ANN1, but with two important distinctions:\n",
    "1) It iteratively produces outputs based on both inputs and previous (recurrent) outputs using shared parameters across time steps\n",
    "2) It employs a \"hidden state\" that is iteratively computed and updated in each forward pass (the hidden state is not a trainable parameter)\n",
    "\n",
    "### Sequence to sequence data\n",
    "RNNs are used mainly for sequential data, i.e. data arranged in sequences where order matters. Typically, this is either time series data, such as weather forecasts or stock markets, or in NLP due to the sequential nature of language. The sequence-to-sequence (seq2seq) image below showcases a comprehensive example of ways sequential data can flow through an RNN.\n",
    "\n",
    "![Seq2seq](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "The leftmost rectangle is a normal feedforward network without RNNs, e.g. your digit predictor from ANN1. <br>\n",
    "The \"one to many\" rectangle takes one input (non-sequential) and produces a sequence of output, e.g. image captioning which takes one image and produces a sequence of text in response. <br>\n",
    "\"many to one\" is the opposite of image captioning, and can be for instance image generatino, where a sequence of text is processed to produce one image. A simpler example is language classification tasks such as sentiment analysis. <br>\n",
    "Finally, there are two examples of \"many to many\". The first example reads the entire input before producing an output, commonly seen in machine translation, where the model reads the whole text in one language to produce a hidden state that represents the latent meaning of the text, then decodes the hidden state to the translated language. The other example of many to many is synced, where each input requires one output in response, such as video frame classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Briefly describe two major challenges and drawbacks with running a standard feed forward ANN (like the one from ANN1) on sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:\n",
    "\n",
    "Standard feed forward ANNs do not retain memory of past inputs, which results in temporal information loss. In sequential data, such as time series or natural language sequences, the order of input elements is crucial for understanding context and making accurate predictions. Without capturing this temporal dependency, feedforward ANNs struggle to model sequential patterns effectively.\r\n",
    "\r\n",
    "Standard feedforward ANN require fixed size inputs and produce fixed-size outputs. However, sequential data often are in variable lengt and would need outputs in variable lengths. A standard feed forward ANNs can not handle such scenarios effectivey..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN structure\n",
    "\n",
    "![rnn-unroll image](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The above image explains the basics of an RNN quite well. Looking at the time-unfolded representation to the right, you'll see how the time-sequential data $x$ is transformed to the outputs $o$. The trainable parameters, the weight matrices in the network that you're building, consist of $U$, $V$, and $W$, where \n",
    "\n",
    "\n",
    "- $U$ defines the connections between the input sample $x_t$ and the current hidden state $h_t$\n",
    "- $V$ defines the connections between the previous hidden state $h_{t-1}$ and $h_t$, and \n",
    "- $W$ defines the connections between the hidden state $h_t$ and the output $o_t$.\n",
    "\n",
    "\n",
    "Note that, as per the seq2seq image above, all parameters are not always used in every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network can thus be represented mathematically as follows:\n",
    "\\begin{equation*}\n",
    "h_t = f(U\\,{x_t} + V\\,{h_{t-1}})\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "o_t = \\mathrm{softmax}(W\\,{h_t})\n",
    "\\tag{2}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A forward pass through an RNN\n",
    "- Let's start at $t = 0$ and take two steps through this netowrk. <br>\n",
    "- Initialise the hidden state h_init with zeros or randomly, $dims: [hidden\\_size, 1]$. <br>\n",
    "- Also initialise the parameters $U$, $V$, and $W$, again with zeros or randomly, since our network is not trained yet.\n",
    "  - $U$ transforms the input to a hidden state contribution $input\\_dim \\rightarrow hidden\\_size$.\n",
    "  - $V$ updates the hidden state $hidden\\_size \\rightarrow hidden\\_size$.\n",
    "  - $W$ transforms the hidden state to the output $hidden\\_size \\rightarrow output\\_dim$. <br>\n",
    "- First, compute the hidden state $h_0$ as a function of $x_0*U$ and $h_{init} * V$. <br>\n",
    "- Then, compute $o_0$ as a function of $h_0$ and $W$. <br>\n",
    "- Then, for the next time step, compute $h_1$ as a function of $x_1*U$ and $h_0*V$. <br>\n",
    "- Finally, compute $o_1$ based on $h_1*W$.<br>\n",
    "- Note that the parameters $U$, $V$ and $W$ remain the same for each iteration. This is because the passing of the sequential data through the same network, as seen to the left of the image, which can be conceptualised as a network with shared weights, as in the right image. <br>\n",
    "- Important: the hidden state $h$ is not learned, but produced during the recurrence. The weights to update the hidden state are learned however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train RNNs?\n",
    "\n",
    "Well, you have to backpropagate through time, which thankfully is not as complicated as it sounds. <br>\n",
    "The final loss between output $o_N$ and label $y_N$ will be a function not only of $x_N$, but of $h_N$. But $h_N$ is a function of $x_{N-1}$ and $h_{N-1}$, which is a function of ... <br>\n",
    "You get the point, the longer the sequence in our network, the longer you have to roll back. <br>\n",
    "\n",
    "### How to backpropagate through time?\n",
    "\n",
    "It's not actually that much different compared to ANN1 if you do it step by step. Starting with the last output, $o_N$ in this case, compute the loss. Then backpropagate from loss to $o_N$, i.e. the derivate of the loss function. Then it is simple to calculate the contribution of $W$ to this loss, i.e. $dL/dW$. This is the contribution of $o_N$ to $W_{grad}$. Save this, but don't update yet! <br>\n",
    "Next, backpropagate into $h_N$, similar to backpropagating into the activation function of $w_1$ in ANN1. Compute $dL/dh_N$, then also backprop through the activation function, as in ANN1. <br>\n",
    "Think of these two steps as going backwards from the last output through $W$ to the last hidden state in the image above. \n",
    "\n",
    "Now, you're inside the hidden state, and you have to backprop in two directions. First, compute $dL/dU$, i.e. follow the $U$ arrow back to the input. <br>\n",
    "Then, compute $dL/dV$, i.e. the contribution of the V parameter to the loss, following the arrow back towards the previous hidden state. Howver, you're not quite done yet, as you also want to compute the contribution of the previous hidden state to the loss, i.e. $dL/dh_{prev}$. Save this for next iteration. <br>\n",
    "Take one time step back, and repeat the process, until you're at $t_0$. <br>\n",
    "\n",
    "### But how does this process take time into account?\n",
    "\n",
    "Note the last derivate you computed, $dL/dh_{prev}$. By solving the partial derivatives you'll find that when computing $dL/dh_{N}$, you relied on the \"fake\" hidden state not yet computed, $dL/dh_{prev}$, which you should initialise to zeros for the first (i.e. last) time step in the backprop. When you compute $dL/dh_{N-1}$ coming from $o_{N-1}$ and $W$, you must include $dL/dh_{prev}$, which this time won't be zero, and corresponds to the backwards movement through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Implementing an RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import packages\n",
    "First, import some packages. We've put all support functions in another file to make the main notebook more readable. If you're interested in how data is loaded and processed, and some very basic NLP, feel free to have a look in the ANN2_support_functions.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import ANN2_support_functions\n",
    "from importlib import reload\n",
    "reload(ANN2_support_functions)\n",
    "from ANN2_support_functions import set_up_datasets, init_orthogonal, one_hot_encode, one_hot_encode_sequence, sequences_to_dicts, set_up_sequences, clip_gradient_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data\n",
    "Let's load the data from the support functions. You'll be using two datasets for this task. First, a \"toy\" dataset with simple generated sequences. The function returns four variables: <br>\n",
    "sequence, which is a list of sequences of characters. The sequences have varying length. <br>\n",
    "char_to_idx, a dict that converts a character to an index. <br>\n",
    "idx_to_char, a dict that converts an index to a character. <br>\n",
    "num_sequences, the number of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the generated dataset:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "There are 100 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The char corresponding to index 1 is 'b'\n",
      "We have 80 samples in the training set.\n",
      "We have 10 samples in the validation set.\n",
      "We have 10 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "sequences, char_to_idx, idx_to_char, num_sequences, vocab_size = set_up_sequences('toy')\n",
    "print('A sentence from the generated dataset:')\n",
    "print(sequences[0])\n",
    "\n",
    "print(f'There are {num_sequences} sentences and {len(char_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', char_to_idx['b'])\n",
    "print(f'The char corresponding to index 1 is \\'{idx_to_char[1]}\\'')\n",
    "\n",
    "training_set, validation_set, test_set = set_up_datasets(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the network\n",
    "Recall the RNN structure from part 1 and use this knowledge to finish the code for network initialisation.\n",
    "\n",
    "**Exercise**: Finish the init_network function below to set up the trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(hidden_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Initialises the RNN parameters\n",
    "    \n",
    "    Args:\n",
    "    `hidden_size`: the dimensions of the hidden state\n",
    "    `vocab_size`: the dimensions of our vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    # Weight matrix (input to hidden state)\n",
    "    # What is the shape of the input and output for this node?\n",
    "    # TODO:\n",
    "    U = np.zeros((hidden_size,vocab_size))\n",
    "    \n",
    "    # Weight matrix (recurrent computation)\n",
    "    # What is the shape of the input and output for this node?\n",
    "    # TODO:\n",
    "    V = np.zeros((hidden_size, hidden_size))\n",
    "    \n",
    "    # Weight matrix (hidden state to output)\n",
    "    # What is the shape of the input and output for this node?\n",
    "    # TODO:\n",
    "    W = np.zeros((vocab_size,hidden_size))\n",
    "    \n",
    "    # Bias (hidden state)\n",
    "    # What is the shape of the input to this node?\n",
    "    # TODO:\n",
    "    b_hidden = np.zeros((hidden_size,1))\n",
    "    \n",
    "    # Bias (output)\n",
    "    # What is the shape of the input to this node?\n",
    "    # TODO:\n",
    "    b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights\n",
    "    # You don't have to change anything here\n",
    "    # Think of it as a better way of doing np.random.rand()\n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)\n",
    "\n",
    "    \n",
    "    parameters = U, V, W, b_hidden, b_out\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the init runs\n",
    "\n",
    "hidden_size = 50 # Number of dimensions in the hidden state\n",
    "vocab_size  = vocab_size # Size of the vocabulary used\n",
    "\n",
    "parameters = init_network(hidden_size=hidden_size, vocab_size=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward pass\n",
    "Here you will implement the forward pass. You'll be using tanh as the activation function for the hidden state update (eq. 1), and softmax as the activation function for the output (eq. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    # Avoid division with zeros\n",
    "    x = x + 1e-12\n",
    "    # The tanh function\n",
    "    a_x = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-a_x**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return a_x\n",
    "    \n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "    \"\"\"\n",
    "    # Avoid division with zeros\n",
    "    x = x + 1e-12\n",
    "    # The softmax function\n",
    "    a_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return a_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Finish the code for the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hidden_state, parameters):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "    `inputs`: sequence of inputs to be processed\n",
    "    `hidden_state`: an already initialized hidden state\n",
    "    `parameters`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First unpack parameters\n",
    "    U, V, W, b_hidden, b_out = parameters\n",
    "    \n",
    "    # Create a list to store outputs and hidden states\n",
    "    outputs, hidden_states = [], []\n",
    "    \n",
    "    # For each element in input sequence\n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # Compute new hidden state\n",
    "        # Hint: look at eq. 1\n",
    "        # TODO:\n",
    "        input_contribution = np.dot(U, inputs[t])\n",
    "        recurrent_contribution = np.dot(V, hidden_state)\n",
    "        hidden_state =  tanh(input_contribution + recurrent_contribution + b_hidden)\n",
    "        \n",
    "        # Compute output\n",
    "        # Hint: Look at eq. 2\n",
    "        # TODO:\n",
    "        out = np.dot(W, hidden_state) + b_out \n",
    "        \n",
    "        # Save results and continue\n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    \n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['UNK', 'UNK', 'UNK', 'b', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'EOS', 'EOS', 'b']\n"
     ]
    }
   ],
   "source": [
    "# Checking the forward pass\n",
    "\n",
    "# Get first sequence in training set\n",
    "test_input_sequence, test_target_sequence = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size, char_to_idx)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Now let's try out our new function\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, parameters)\n",
    "\n",
    "print('Input sequence:')\n",
    "print(test_input_sequence)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(test_target_sequence)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_char[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backward pass\n",
    "**Exercise:** Finish the code for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, outputs, hidden_states, targets, parameters):\n",
    "    \"\"\"\n",
    "    Computes the backward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "     `inputs`: sequence of inputs to be processed\n",
    "     `outputs`: sequence of outputs from the forward pass\n",
    "     `hidden_states`: sequence of hidden_states from the forward pass\n",
    "     `targets`: sequence of targets\n",
    "     `params`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First unpack parameters\n",
    "    U, V, W, b_hidden, b_out = parameters\n",
    "    \n",
    "    # Initialize gradients as zero\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out) \n",
    "\n",
    "    \n",
    "    # Keep track of hidden state derivative and loss\n",
    "    d_h_prev = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    # For each element in output sequence\n",
    "    # iterate backwards over t -> t = N, N-1, ... 1, 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        #loss += -np.mean(np.log(outputs[t] + 1e-12) * targets[t])\n",
    "        epsilon = 1e-12\n",
    "        loss += -np.mean(np.log(np.clip(outputs[t], epsilon, 1.0)) * targets[t])\n",
    "        \n",
    "        \n",
    "        # Backpropagate into output (derivative of cross-entropy and softmax)\n",
    "        # dL/do cancels out all annoying parts of dL/dz * dz/do into a simple function\n",
    "        # please see https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "        # for more details\n",
    "        # Also the article https://towardsdatascience.com/backpropagation-in-rnn-explained-bdf853b4e1c2 was helpful to understand backpropogation in RNN.\n",
    "        d_o = outputs[t].copy()\n",
    "        d_o[np.argmax(targets[t])] -= 1\n",
    "        \n",
    "        # Backpropagate into W\n",
    "        # TODO:\n",
    "        d_W += np.dot(d_o, hidden_states[t].T)\n",
    "        \n",
    "        # Backpropagate into bias\n",
    "        d_b_out += d_o\n",
    "        \n",
    "        # Backpropagate into h\n",
    "        # TODO:\n",
    "        d_h = np.dot(W.T, d_o) + d_h_prev \n",
    "        \n",
    "        # Backpropagate through the activation function\n",
    "        # Hint: the tanh function can be called using derivative=True\n",
    "        # TODO: \n",
    "        d_a = d_h * tanh(hidden_states[t], derivative=True)\n",
    "\n",
    "        # Backpropagate into bias\n",
    "        d_b_hidden += d_a\n",
    "        \n",
    "        # Backpropagate into U\n",
    "        # TODO:\n",
    "        d_U += np.dot(d_a, inputs[t].T)\n",
    "        \n",
    "        # Backpropagate into V\n",
    "        # TODO:\n",
    "        d_V += np.dot(d_a, hidden_states[t-1].T)\n",
    "\n",
    "        # Backpropagate into previous hidden state\n",
    "        d_h_prev = np.dot(V.T, d_a)\n",
    "    \n",
    "    # Pack gradients\n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    \n",
    "    # Clip gradients to prevent explosions\n",
    "    grads = clip_gradient_norm(grads)\n",
    "\n",
    "    # Calculate the mean loss\n",
    "    loss = loss/len(outputs)\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of:\n",
      "2.7660866848094896\n"
     ]
    }
   ],
   "source": [
    "# Checking the backwards pass\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, gradients = backward_pass(test_input, outputs, hidden_states, test_target, parameters)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimisation algorithm\n",
    "**Exercise:** Finish the code for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, gradients, lr=1e-3):\n",
    "    # Take a step\n",
    "    updated_params = []\n",
    "    for parameter, gradient in zip(parameters, gradients):\n",
    "        #TODO: \n",
    "        parameter -= gradient * lr\n",
    "        updated_params.append(parameter)\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters seem to update during backprop\n"
     ]
    }
   ],
   "source": [
    "# Check that our gradient descent works\n",
    "# Ignore all the arcane unrolling of lists. All this function does is to check that your update function actually changes the parameters\n",
    "\n",
    "# Copy old parameters so we can check that they've updated\n",
    "prev_parameters = [parameter.copy() for parameter in parameters]\n",
    "\n",
    "# Use computed gradients to update parameters\n",
    "parameters = gradient_descent(parameters, gradients, lr=3e-4)\n",
    "\n",
    "# Check that parameters have updated\n",
    "parameters_delta = [parameter - prev_parameter for parameter, prev_parameter in zip(parameters, prev_parameters)]\n",
    "\n",
    "assert all([parameter.any() for parameter in parameters_delta]), \"parameters have not updated\"\n",
    "print(\"parameters seem to update during backprop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "**Exercise:** Implement all individual steps together to loop over the dataset. Train until you have a loss under 2 or an accuracy over 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 5.494513915563995, validation loss: 5.6188619630861805\n",
      "Epoch 100, training loss: 0.3691052184181068, validation loss: 0.5023631468634078\n",
      "Epoch 200, training loss: 0.19203140924288412, validation loss: 0.20970453255719365\n",
      "Epoch 300, training loss: 0.1695040191731993, validation loss: 0.18557970696896006\n",
      "Epoch 400, training loss: 0.15735937121187582, validation loss: 0.17213241664758966\n",
      "Epoch 500, training loss: 0.14849622102085008, validation loss: 0.1620969929872076\n",
      "Epoch 600, training loss: 0.14159148099778893, validation loss: 0.15419343368989952\n",
      "Epoch 700, training loss: 0.13621533459997476, validation loss: 0.14780511508571081\n",
      "Epoch 800, training loss: 0.1318843348514003, validation loss: 0.14234161076466684\n",
      "Epoch 900, training loss: 0.12862605272776567, validation loss: 0.13790583400752854\n",
      "Epoch 1000, training loss: 0.12625450061807272, validation loss: 0.13452166617975383\n",
      "Epoch 1100, training loss: 0.12440025049158195, validation loss: 0.13182872903566642\n",
      "Epoch 1200, training loss: 0.12293677283220195, validation loss: 0.12963400629986443\n",
      "Epoch 1300, training loss: 0.12172991512095521, validation loss: 0.12780143008038874\n",
      "Epoch 1400, training loss: 0.12071911379791571, validation loss: 0.12625879332293466\n",
      "Epoch 1500, training loss: 0.11981147485795177, validation loss: 0.12488143820475206\n",
      "Epoch 1600, training loss: 0.11897738304543268, validation loss: 0.1236236113725813\n",
      "Epoch 1700, training loss: 0.11819561501662042, validation loss: 0.12245740183130956\n",
      "Epoch 1800, training loss: 0.11745351220196061, validation loss: 0.12137725105398796\n",
      "Epoch 1900, training loss: 0.11674557667705099, validation loss: 0.12035393265343194\n",
      "Epoch 2000, training loss: 0.11605897287514662, validation loss: 0.11938747758206969\n",
      "Epoch 2100, training loss: 0.11538350439672426, validation loss: 0.11845603161983705\n",
      "Epoch 2200, training loss: 0.11471614783337344, validation loss: 0.11755432173622701\n",
      "Epoch 2300, training loss: 0.1140652916571085, validation loss: 0.1166787623176601\n",
      "Epoch 2400, training loss: 0.1134272466782628, validation loss: 0.1158267694577922\n",
      "Epoch 2500, training loss: 0.11279604686779628, validation loss: 0.11500302026679847\n",
      "Epoch 2600, training loss: 0.1121625223125764, validation loss: 0.11419822648582781\n",
      "Epoch 2700, training loss: 0.11152262192770421, validation loss: 0.11340845896502935\n",
      "Epoch 2800, training loss: 0.110870686948533, validation loss: 0.11262996972678305\n",
      "Epoch 2900, training loss: 0.11019946449253662, validation loss: 0.11185815259583867\n",
      "Epoch 3000, training loss: 0.10950093994582169, validation loss: 0.11108812118674924\n",
      "Epoch 3100, training loss: 0.1087692700721578, validation loss: 0.11031671084084813\n",
      "Epoch 3200, training loss: 0.10800563097246303, validation loss: 0.10954552429512601\n",
      "Epoch 3300, training loss: 0.10722160214544392, validation loss: 0.10878263635693541\n",
      "Epoch 3400, training loss: 0.1064377498882508, validation loss: 0.1080410435416631\n",
      "Epoch 3500, training loss: 0.10567774065858196, validation loss: 0.10733452829030614\n",
      "Epoch 3600, training loss: 0.10496120501559428, validation loss: 0.10667328305974544\n",
      "Epoch 3700, training loss: 0.10429900629713733, validation loss: 0.10606158059205562\n",
      "Epoch 3800, training loss: 0.10370173838255539, validation loss: 0.10549824957770874\n",
      "Epoch 3900, training loss: 0.1031830760763337, validation loss: 0.10499506954045279\n",
      "Epoch 4000, training loss: 0.10270811451266322, validation loss: 0.10453568387271332\n",
      "Epoch 4100, training loss: 0.1022970046325041, validation loss: 0.10413046235695098\n",
      "Epoch 4200, training loss: 0.10192095588416403, validation loss: 0.10376149560572796\n",
      "Epoch 4300, training loss: 0.10157564408571691, validation loss: 0.10341679382308014\n",
      "Epoch 4400, training loss: 0.10126217867561868, validation loss: 0.10309450991375964\n",
      "Epoch 4500, training loss: 0.10098756747205155, validation loss: 0.10280691072268178\n",
      "Epoch 4600, training loss: 0.10075560852687018, validation loss: 0.10258687489854648\n",
      "Epoch 4700, training loss: 0.10054905958460743, validation loss: 0.10238619228917185\n",
      "Epoch 4800, training loss: 0.10036547543308764, validation loss: 0.10220438256766369\n",
      "Epoch 4900, training loss: 0.1002044320091885, validation loss: 0.10204092484551353\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 5000 \n",
    "hidden_size = 50\n",
    "\n",
    "# Initialize a new network\n",
    "parameters = init_network(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Keep track of best validation loss\n",
    "min_loss = 10000\n",
    "best_parameters = None\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "     # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO:\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "        # Backward pass\n",
    "        # TODO:\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "\n",
    "        # If lowest val loss, save parameters of model\n",
    "        if loss<min_loss:\n",
    "            min_loss = loss\n",
    "            best_parameters = parameters\n",
    "\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO: \n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "        \n",
    "        # Backward pass\n",
    "        # TODO:\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "        \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradients have vanished!')\n",
    "        \n",
    "        # Update parameters\n",
    "        # TODO:\n",
    "        parameters = gradient_descent(parameters, grads, lr=3e-4)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if i % 100  == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "\n",
    "    # Use best parameters for making predictions\n",
    "    parameters = best_parameters \n",
    "\n",
    "# Save parameters to a file for later use, use pickle\n",
    "with open('best_parameters_simpleSeq.pkl', 'wb') as f:\n",
    "    pickle.dump(best_parameters, f)\n",
    "\n",
    "# Get first sentence in train set\n",
    "inputs, targets = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "# TODO:\n",
    "outputs, _ = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "# Convert output to character\n",
    "output_sentence = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_char[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "['a', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'EOS']\n",
      "Test accuracy is 50.0%.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Testing your network\n",
    "\n",
    "# Load best parameters using pickle\n",
    "#parameters = best_parameters\n",
    "with open('best_parameters_simpleSeq.pkl', 'rb') as f:\n",
    "    parameters = pickle.load(f)\n",
    "\n",
    "\n",
    "# Get first sequence in testing set\n",
    "test_input_sequence, test_target_sequence = test_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size, char_to_idx)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "# TODO:\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, parameters)\n",
    "\n",
    "print('Input sequence:')\n",
    "print(test_input_sequence)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(test_target_sequence)\n",
    "\n",
    "preds = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "print('\\nPredicted sequence:')\n",
    "print(preds)\n",
    "\n",
    "accuracy = 0\n",
    "for target, pred in zip(test_target_sequence, preds):\n",
    "    accuracy += target == pred\n",
    "accuracy /= len(test_target_sequence)/100\n",
    "\n",
    "print(f\"Test accuracy is {np.round(accuracy)}%.\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "['a', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'EOS']\n",
      "Test accuracy is 50.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 79.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 79.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 75.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 90.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 100.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 75.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 83.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 100.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'EOS']\n",
      "Test accuracy is 50.0%.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing with whole test set\n",
    "\n",
    "# Load best parameters using pickle\n",
    "with open('best_parameters_simpleSeq.pkl', 'rb') as f:\n",
    "    parameters = pickle.load(f)\n",
    "\n",
    "# For each sentence in test set\n",
    "for inputs, targets in test_set:\n",
    "\n",
    "    # One-hot encode input and target sequence\n",
    "    test_input = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "    test_target = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "    # Initialize hidden state as zeros\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Forward pass\n",
    "    # TODO:\n",
    "    outputs, _ = forward_pass(test_input, hidden_state, parameters)\n",
    "\n",
    "    print('Input sequence:')\n",
    "    print(inputs)\n",
    "\n",
    "    print('\\nTarget sequence:')\n",
    "    print(targets)\n",
    "\n",
    "    preds = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "    print('\\nPredicted sequence:')\n",
    "    print(preds)\n",
    "\n",
    "    accuracy = 0\n",
    "    for target, pred in zip(targets, preds):\n",
    "        accuracy += target == pred\n",
    "    accuracy /= len(targets)/100\n",
    "    \n",
    "    print(f\"Test accuracy is {np.round(accuracy)}%.\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a more complex task\n",
    "The you dataset is an extremely simple set of characters to predict. Now, let's try using a more complicated dataset. The code below imports data from a collection of Shakespeare's plays and turns it into sequences of characters. Try running your network on this data and see if it works.\n",
    "\n",
    "**Exercise:** Paste and run your training and testing on the Shakespeare dataset. You don't have to achieve any specific loss or accuracy. Does it work? Please speculate on why/why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here:\n",
    "Training and Validation:\n",
    "It seems to work well. Althouh it took many epochs to converge. As seen in the graph, with the simple sequence the training and the validation loss were following each other. On the other hand, with the Shakespeare dataset although both validation and training loss were following the pattern but they were not exactly the same. We think that this could improve if larger data is used for the training.  \n",
    "\n",
    "Test:\n",
    "Accuracy tested on the first test sequence is not so good. Just 28%. On testing whole test set, the best accuracy is 29%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the Shakespeare dataset: ['f', 'i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k', 'EOS']\n",
      "We have 80 samples in the training set.\n",
      "We have 10 samples in the validation set.\n",
      "We have 10 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "sequences, char_to_idx, idx_to_char, num_sequences, vocab_size = set_up_sequences()\n",
    "\n",
    "#num_sequences = len(sequences)\n",
    "#vocab_size = len(np.unique(sequences))\n",
    "print(f'A sentence from the Shakespeare dataset: {sequences[0]}')\n",
    "\n",
    "# Whole dataset is too big to effectively train on, so let's start by grabbing the first 100 sequences\n",
    "sequences = sequences[0:100]\n",
    "training_set, validation_set, test_set = set_up_datasets(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Your code for training and testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.3827757209651854, validation loss: 0.39207086770907024\n",
      "Epoch 100, training loss: 0.3739523085329285, validation loss: 0.3999463058815743\n",
      "Epoch 200, training loss: 0.3419531349114352, validation loss: 0.37611851902440263\n",
      "Epoch 300, training loss: 0.31024500420646267, validation loss: 0.3458291195027735\n",
      "Epoch 400, training loss: 0.28742444402428624, validation loss: 0.3146917188200015\n",
      "Epoch 500, training loss: 0.26346960639672445, validation loss: 0.2977832957554168\n",
      "Epoch 600, training loss: 0.24244505568618901, validation loss: 0.27782351217195106\n",
      "Epoch 700, training loss: 0.22163274469024458, validation loss: 0.2571298757692253\n",
      "Epoch 800, training loss: 0.20671231485138275, validation loss: 0.24266860616943195\n",
      "Epoch 900, training loss: 0.19086117150949597, validation loss: 0.22971745005039607\n",
      "Epoch 1000, training loss: 0.17791260680798426, validation loss: 0.20510316734920991\n",
      "Epoch 1100, training loss: 0.16607039896183595, validation loss: 0.18711308305342925\n",
      "Epoch 1200, training loss: 0.15896915506656614, validation loss: 0.1737789633287969\n",
      "Epoch 1300, training loss: 0.14951649092550956, validation loss: 0.15740429515421064\n",
      "Epoch 1400, training loss: 0.1414768106710536, validation loss: 0.15023799568709884\n",
      "Epoch 1500, training loss: 0.13378790297951115, validation loss: 0.145178134800867\n",
      "Epoch 1600, training loss: 0.12871934867138904, validation loss: 0.13966578718056433\n",
      "Epoch 1700, training loss: 0.12385763584023142, validation loss: 0.13518065157755946\n",
      "Epoch 1800, training loss: 0.11951692058812553, validation loss: 0.12865426385739623\n",
      "Epoch 1900, training loss: 0.11580147103537808, validation loss: 0.12564544295765515\n",
      "Epoch 2000, training loss: 0.11256357310560809, validation loss: 0.12070092562560797\n",
      "Epoch 2100, training loss: 0.1096969149691636, validation loss: 0.11937242627876261\n",
      "Epoch 2200, training loss: 0.10746578318315972, validation loss: 0.11552038513891245\n",
      "Epoch 2300, training loss: 0.1049909323995776, validation loss: 0.11414565868603921\n",
      "Epoch 2400, training loss: 0.10390054458628133, validation loss: 0.11241232726720037\n",
      "Epoch 2500, training loss: 0.102718585855385, validation loss: 0.11142351131030952\n",
      "Epoch 2600, training loss: 0.1013415109832972, validation loss: 0.11105834865263076\n",
      "Epoch 2700, training loss: 0.10051480843578793, validation loss: 0.11257145381654106\n",
      "Epoch 2800, training loss: 0.09880157847386636, validation loss: 0.11220793722474776\n",
      "Epoch 2900, training loss: 0.09783228855425054, validation loss: 0.11220883461823328\n",
      "Epoch 3000, training loss: 0.09696277555298663, validation loss: 0.11168659037232301\n",
      "Epoch 3100, training loss: 0.09602901332553299, validation loss: 0.11116790333044677\n",
      "Epoch 3200, training loss: 0.09513848781901321, validation loss: 0.11083799270206991\n",
      "Epoch 3300, training loss: 0.09429954032000358, validation loss: 0.11040480285926406\n",
      "Epoch 3400, training loss: 0.09336135581732488, validation loss: 0.10997328766522965\n",
      "Epoch 3500, training loss: 0.09299822234107133, validation loss: 0.10985452437622242\n",
      "Epoch 3600, training loss: 0.09223324984753348, validation loss: 0.1092392153245711\n",
      "Epoch 3700, training loss: 0.09127759096321131, validation loss: 0.10913645062709458\n",
      "Epoch 3800, training loss: 0.09085175124365474, validation loss: 0.10905622472782579\n",
      "Epoch 3900, training loss: 0.09052819357691766, validation loss: 0.10538817122690639\n",
      "Epoch 4000, training loss: 0.09011270597732345, validation loss: 0.1051374078576317\n",
      "Epoch 4100, training loss: 0.08979087695471837, validation loss: 0.10556967129850055\n",
      "Epoch 4200, training loss: 0.08960126080465683, validation loss: 0.10586251821377021\n",
      "Epoch 4300, training loss: 0.0895222594922355, validation loss: 0.10576970642027284\n",
      "Epoch 4400, training loss: 0.08860129078993988, validation loss: 0.1056937967602537\n",
      "Epoch 4500, training loss: 0.08816962078234801, validation loss: 0.10523233696019767\n",
      "Epoch 4600, training loss: 0.08748518261298302, validation loss: 0.10440915902164882\n",
      "Epoch 4700, training loss: 0.08720282795472159, validation loss: 0.10429802739564169\n",
      "Epoch 4800, training loss: 0.08671839025852336, validation loss: 0.10403040786182552\n",
      "Epoch 4900, training loss: 0.08632385642670519, validation loss: 0.10321906644858472\n",
      "Input sentence:\n",
      "['f', 'i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['i', 'r', 's', 't', 'h', 'i', 't', 'h', 'n', 'e', 'n', 'o', 'e', 'n', 'i', 'r', 'e', 'n', 'h', 'r', 'e', 'e', 'u', 'i', 'n', 'n', 'e', 'n', 'd', 'o', 'u', 'r', 's', 'h', 'e', 's', 'e', 'e', 's', 't', 'e', 'a', 's', 't', 'e', 'a', 't', 'e']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training and validation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 5000 \n",
    "hidden_size = 50\n",
    "\n",
    "# Initialize a new network\n",
    "parameters = init_network(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Keep track of best validation loss\n",
    "min_loss = 10000\n",
    "best_parameters = None\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "     # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO:\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "        # Backward pass\n",
    "        # TODO:\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "\n",
    "        # If lowest val loss, save parameters of model\n",
    "        if loss<min_loss:\n",
    "            min_loss = loss\n",
    "            best_parameters = parameters\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO: \n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "        \n",
    "        # Backward pass\n",
    "        # TODO:\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "        \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradients have vanished!')\n",
    "        \n",
    "        # Update parameters\n",
    "        # TODO:\n",
    "        parameters = gradient_descent(parameters, grads, lr=3e-4)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if i % 100  == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "\n",
    "    # Use best parameters for making predictions\n",
    "    parameters = best_parameters \n",
    "\n",
    "\n",
    "# Save best parameters to a file for later use, use pickle\n",
    "with open('best_parameters_rnn_shakespeare.pkl', 'wb') as f:\n",
    "    pickle.dump(best_parameters, f)\n",
    "\n",
    "# Get first sentence in train set\n",
    "inputs, targets = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "# TODO:\n",
    "outputs, _ = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "# Convert output to character\n",
    "output_sentence = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_char[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "[\"'\", 't', 'h', 'e', 'y', 'h', 'a', 'v', 'e', 'p', 'r', 'e', 's', 's', \"'\", 'd', 'a', 'p', 'o', 'w', 'e', 'r', 'b', 'u', 't', 'i', 't', 'i', 's', 'n', 'o', 't', 'k', 'n', 'o', 'w', 'n', 'w', 'h', 'e', 't', 'h', 'e', 'r', 'f', 'o', 'r', 'e', 'a', 's', 't', 'o', 'r', 'w', 'e', 's', 't', 't', 'h', 'e', 'd', 'e', 'a', 'r', 't', 'h', 'i', 's', 'g', 'r', 'e', 'a', 't', ';', 't', 'h', 'e', 'p', 'e', 'o', 'p', 'l', 'e', 'm', 'u', 't', 'i', 'n', 'o', 'u', 's', ';', 'a', 'n', 'd', 'i', 't', 'i', 's', 'r', 'u', 'm', 'o', 'u', 'r', \"'\", 'd', 'c', 'o', 'm', 'i', 'n', 'i', 'u', 's', 'm', 'a', 'r', 'c', 'i', 'u', 's', 'y', 'o', 'u', 'r', 'o', 'l', 'd', 'e', 'n', 'e', 'm', 'y', 'w', 'h', 'o', 'i', 's', 'o', 'f', 'r', 'o', 'm', 'e', 'w', 'o', 'r', 's', 'e', 'h', 'a', 't', 'e', 'd', 't', 'h', 'a', 'n', 'o', 'f', 'y', 'o', 'u', 'a', 'n', 'd', 't', 'i', 't', 'u', 's', 'l', 'a', 'r', 't', 'i', 'u', 's', 'a', 'm', 'o', 's', 't', 'v', 'a', 'l', 'i', 'a', 'n', 't', 'r', 'o', 'm', 'a', 'n', 't', 'h', 'e', 's', 'e', 't', 'h', 'r', 'e', 'e', 'l', 'e', 'a', 'd', 'o', 'n', 't', 'h', 'i', 's', 'p', 'r', 'e', 'p', 'a', 'r', 'a', 't', 'i', 'o', 'n', 'w', 'h', 'i', 't', 'h', 'e', 'r', \"'\", 't', 'i', 's', 'b', 'e', 'n', 't', 'm', 'o', 's', 't', 'l', 'i', 'k', 'e', 'l', 'y', \"'\", 't', 'i', 's', 'f', 'o', 'r', 'y', 'o', 'u', 'c', 'o', 'n', 's', 'i', 'd', 'e', 'r', 'o', 'f', 'i', 't']\n",
      "\n",
      "Target sequence:\n",
      "['t', 'h', 'e', 'y', 'h', 'a', 'v', 'e', 'p', 'r', 'e', 's', 's', \"'\", 'd', 'a', 'p', 'o', 'w', 'e', 'r', 'b', 'u', 't', 'i', 't', 'i', 's', 'n', 'o', 't', 'k', 'n', 'o', 'w', 'n', 'w', 'h', 'e', 't', 'h', 'e', 'r', 'f', 'o', 'r', 'e', 'a', 's', 't', 'o', 'r', 'w', 'e', 's', 't', 't', 'h', 'e', 'd', 'e', 'a', 'r', 't', 'h', 'i', 's', 'g', 'r', 'e', 'a', 't', ';', 't', 'h', 'e', 'p', 'e', 'o', 'p', 'l', 'e', 'm', 'u', 't', 'i', 'n', 'o', 'u', 's', ';', 'a', 'n', 'd', 'i', 't', 'i', 's', 'r', 'u', 'm', 'o', 'u', 'r', \"'\", 'd', 'c', 'o', 'm', 'i', 'n', 'i', 'u', 's', 'm', 'a', 'r', 'c', 'i', 'u', 's', 'y', 'o', 'u', 'r', 'o', 'l', 'd', 'e', 'n', 'e', 'm', 'y', 'w', 'h', 'o', 'i', 's', 'o', 'f', 'r', 'o', 'm', 'e', 'w', 'o', 'r', 's', 'e', 'h', 'a', 't', 'e', 'd', 't', 'h', 'a', 'n', 'o', 'f', 'y', 'o', 'u', 'a', 'n', 'd', 't', 'i', 't', 'u', 's', 'l', 'a', 'r', 't', 'i', 'u', 's', 'a', 'm', 'o', 's', 't', 'v', 'a', 'l', 'i', 'a', 'n', 't', 'r', 'o', 'm', 'a', 'n', 't', 'h', 'e', 's', 'e', 't', 'h', 'r', 'e', 'e', 'l', 'e', 'a', 'd', 'o', 'n', 't', 'h', 'i', 's', 'p', 'r', 'e', 'p', 'a', 'r', 'a', 't', 'i', 'o', 'n', 'w', 'h', 'i', 't', 'h', 'e', 'r', \"'\", 't', 'i', 's', 'b', 'e', 'n', 't', 'm', 'o', 's', 't', 'l', 'i', 'k', 'e', 'l', 'y', \"'\", 't', 'i', 's', 'f', 'o', 'r', 'y', 'o', 'u', 'c', 'o', 'n', 's', 'i', 'd', 'e', 'r', 'o', 'f', 'i', 't', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['s', 'h', 'e', 'n', 'o', 'e', 't', 'e', 'n', 'e', 'e', 'a', 't', 't', 't', 'e', 't', 'e', 'u', 'h', 'r', 'e', 'e', 's', 'h', 'n', 'h', 'n', 't', 'o', 'u', 'h', 'e', 'o', 'u', 'h', 'o', 'e', 'e', 's', 'h', 'e', 's', 'e', 'i', 'r', 'e', 'n', 'r', 't', 'h', 'u', 'e', 'h', 'r', 't', 'h', 'h', 'e', 'n', 'e', 'r', 't', 'e', 'h', 'e', 't', 'e', 'o', 'e', 'n', 't', 'h', 'r', 'h', 'e', 'r', 'e', 'a', 'u', 'e', 'e', 'n', 'e', 's', 'h', 'n', 's', 'u', 's', 't', 'r', 'r', 'd', 't', 'n', 'h', 't', 't', 'e', 's', 'a', 'u', 's', 's', 't', 'e', 'i', 'u', 'a', 'n', 'e', 't', 's', 't', 'a', 'r', 'e', 'i', 'u', 's', 't', 'o', 'u', 's', 'e', 'u', 'l', 'e', 'r', 'o', 'n', 'a', 'o', 'h', 'e', 'u', 't', 'e', 'u', 'i', 'e', 'u', 'a', 'n', 'h', 'u', 'e', 't', 'r', 'e', 't', 'h', 'n', 'e', 'h', 'e', 't', 'd', 'u', 'i', 'o', 'u', 's', 'n', 'd', 't', 'h', 't', 'h', 's', 't', 'l', 't', 'e', 'h', 'n', 's', 't', 't', 'e', 'u', 't', 'h', 'e', 't', 'l', 't', 'n', 'd', 'h', 'e', 'u', 'e', 'n', 'd', 'h', 'e', 's', 't', 'r', 'h', 'e', 'e', 'a', 's', 'l', 'n', 't', 'e', 'u', 't', 'h', 'e', 't', 'e', 'e', 'e', 'a', 'e', 't', 'e', 't', 'h', 't', 'u', 't', 'h', 'e', 't', 'h', 'e', 's', 'e', 't', 'h', 't', 't', 'e', 'r', 'e', 'h', 'e', 'u', 't', 'h', 'e', 't', 'e', 's', 'l', 'o', 't', 'h', 't', 't', 'i', 'r', 'e', 'o', 'u', 's', 'i', 'u', 'd', 't', 't', 'e', 'n', 'e', 'u', 'i', 'r', 'h']\n",
      "Test accuracy is 28.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "[\"'\", 'f', 'i', 'r', 's', 't', 's', 'e', 'n', 'a', 't', 'o', 'r', 'o', 'u', 'r', 'a', 'r', 'm', 'y', \"'\", 's', 'i', 'n', 't', 'h', 'e', 'f', 'i', 'e', 'l', 'd', 'w', 'e', 'n', 'e', 'v', 'e', 'r', 'y', 'e', 't', 'm', 'a', 'd', 'e', 'd', 'o', 'u', 'b', 't', 'b', 'u', 't', 'r', 'o', 'm', 'e', 'w', 'a', 's', 'r', 'e', 'a', 'd', 'y', 't', 'o', 'a', 'n', 's', 'w', 'e', 'r', 'u', 's']\n",
      "\n",
      "Target sequence:\n",
      "['f', 'i', 'r', 's', 't', 's', 'e', 'n', 'a', 't', 'o', 'r', 'o', 'u', 'r', 'a', 'r', 'm', 'y', \"'\", 's', 'i', 'n', 't', 'h', 'e', 'f', 'i', 'e', 'l', 'd', 'w', 'e', 'n', 'e', 'v', 'e', 'r', 'y', 'e', 't', 'm', 'a', 'd', 'e', 'd', 'o', 'u', 'b', 't', 'b', 'u', 't', 'r', 'o', 'm', 'e', 'w', 'a', 's', 'r', 'e', 'a', 'd', 'y', 't', 'o', 'a', 'n', 's', 'w', 'e', 'r', 'u', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['s', 'i', 'r', 's', 't', 'h', 'e', 'n', 'o', 't', 'h', 'u', 'e', 'u', 's', 's', 't', 'e', 'a', 'o', 't', 't', 't', 'e', 'h', 'e', 's', 'i', 'r', 's', 'l', 'e', 'h', 'r', 'd', 'n', 'e', 'n', 'e', 'o', 'u', 'h', 'e', 'n', 'e', 'n', 'e', 'u', 's', 'e', 'h', 'e', 's', 'h', 'e', 'u', 'e', 's', 'h', 'r', 't', 'e', 'a', 't', 'e', 'o', 'h', 'u', 'n', 'd', 't', 'e', 'r', 'e', 's', 't']\n",
      "Test accuracy is 28.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 'n', 'o', 'r', 'd', 'i', 'd', 'y', 'o', 'u', 't', 'h', 'i', 'n', 'k', 'i', 't', 'f', 'o', 'l', 'l', 'y', 't', 'o', 'k', 'e', 'e', 'p', 'y', 'o', 'u', 'r', 'g', 'r', 'e', 'a', 't', 'p', 'r', 'e', 't', 'e', 'n', 'c', 'e', 's', 'v', 'e', 'i', 'l', \"'\", 'd', 't', 'i', 'l', 'l', 'w', 'h', 'e', 'n', 't', 'h', 'e', 'y', 'n', 'e', 'e', 'd', 's', 'm', 'u', 's', 't', 's', 'h', 'o', 'w', 't', 'h', 'e', 'm', 's', 'e', 'l', 'v', 'e', 's', ';', 'w', 'h', 'i', 'c', 'h', 'i', 'n', 't', 'h', 'e', 'h', 'a', 't', 'c', 'h', 'i', 'n', 'g', 'i', 't', 's', 'e', 'e', 'm', \"'\", 'd', 'a', 'p', 'p', 'e', 'a', 'r', \"'\", 'd', 't', 'o', 'r', 'o', 'm', 'e']\n",
      "\n",
      "Target sequence:\n",
      "['u', 'f', 'i', 'd', 'i', 'u', 's', 'n', 'o', 'r', 'd', 'i', 'd', 'y', 'o', 'u', 't', 'h', 'i', 'n', 'k', 'i', 't', 'f', 'o', 'l', 'l', 'y', 't', 'o', 'k', 'e', 'e', 'p', 'y', 'o', 'u', 'r', 'g', 'r', 'e', 'a', 't', 'p', 'r', 'e', 't', 'e', 'n', 'c', 'e', 's', 'v', 'e', 'i', 'l', \"'\", 'd', 't', 'i', 'l', 'l', 'w', 'h', 'e', 'n', 't', 'h', 'e', 'y', 'n', 'e', 'e', 'd', 's', 'm', 'u', 's', 't', 's', 'h', 'o', 'w', 't', 'h', 'e', 'm', 's', 'e', 'l', 'v', 'e', 's', ';', 'w', 'h', 'i', 'c', 'h', 'i', 'n', 't', 'h', 'e', 'h', 'a', 't', 'c', 'h', 'i', 'n', 'g', 'i', 't', 's', 'e', 'e', 'm', \"'\", 'd', 'a', 'p', 'p', 'e', 'a', 'r', \"'\", 'd', 't', 'o', 'r', 'o', 'm', 'e', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['r', 's', 'i', 'r', 's', 't', 's', 't', 'd', 'u', 'e', 'e', 's', 'e', 'o', 'u', 's', 'h', 'e', 't', 's', 'e', 's', 'h', 'o', 'r', 'l', 'l', 'o', 'h', 'u', 'e', 's', 'r', 'e', 'o', 'u', 's', 'e', 'i', 'e', 's', 't', 'h', 'e', 'e', 'n', 'h', 'n', 'o', 'o', 'n', 'i', 'e', 'n', 't', 'l', 't', 'e', 'h', 't', 'l', 'l', 'h', 'e', 's', 'o', 'h', 'e', 's', 'o', 'u', 'a', 'n', 'i', 'e', 'e', 's', 't', 'h', 'e', 'e', 'u', 'h', 'h', 'e', 'n', 'e', 't', 'n', 'l', 'e', 'n', 't', 'r', 'h', 'e', 't', 'i', 'e', 's', 'e', 'h', 'e', 's', 'e', 't', 'h', 'i', 'e', 's', 'e', 'o', 't', 'h', 't', 'n', 'r', 'e', 's', 'e', 't', 'e', 'e', 'a', 'r', 'e', 't', 'e', 'h', 'u', 'e', 'u', 'e', 'n']\n",
      "Test accuracy is 25.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['b', 'y', 't', 'h', 'e', 'd', 'i', 's', 'c', 'o', 'v', 'e', 'r', 'y']\n",
      "\n",
      "Target sequence:\n",
      "['y', 't', 'h', 'e', 'd', 'i', 's', 'c', 'o', 'v', 'e', 'r', 'y', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'o', 'h', 'e', 's', 'e', 'n', 't', 'i', 'u', 'e', 'n', 'e', 'o']\n",
      "Test accuracy is 21.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['w', 'e', 's', 'h', 'a', 'l', 'l', 'b', 'e', 's', 'h', 'o', 'r', 't', 'e', 'n', \"'\", 'd', 'i', 'n', 'o', 'u', 'r', 'a', 'i', 'm', 'w', 'h', 'i', 'c', 'h', 'w', 'a', 's', 't', 'o', 't', 'a', 'k', 'e', 'i', 'n', 'm', 'a', 'n', 'y', 't', 'o', 'w', 'n', 's', 'e', 'r', 'e', 'a', 'l', 'm', 'o', 's', 't', 'r', 'o', 'm', 'e', 's', 'h', 'o', 'u', 'l', 'd', 'k', 'n', 'o', 'w', 'w', 'e', 'w', 'e', 'r', 'e', 'a', 'f', 'o', 'o', 't']\n",
      "\n",
      "Target sequence:\n",
      "['e', 's', 'h', 'a', 'l', 'l', 'b', 'e', 's', 'h', 'o', 'r', 't', 'e', 'n', \"'\", 'd', 'i', 'n', 'o', 'u', 'r', 'a', 'i', 'm', 'w', 'h', 'i', 'c', 'h', 'w', 'a', 's', 't', 'o', 't', 'a', 'k', 'e', 'i', 'n', 'm', 'a', 'n', 'y', 't', 'o', 'w', 'n', 's', 'e', 'r', 'e', 'a', 'l', 'm', 'o', 's', 't', 'r', 'o', 'm', 'e', 's', 'h', 'o', 'u', 'l', 'd', 'k', 'n', 'o', 'w', 'w', 'e', 'w', 'e', 'r', 'e', 'a', 'f', 'o', 'o', 't', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['h', 'r', 't', 'e', 't', 'l', 'l', 'e', 'n', 't', 'e', 'u', 'e', 'h', 'r', 'e', 't', 'a', 's', 's', 'u', 's', 's', 't', 't', 'e', 'h', 'e', 't', 'i', 'e', 'h', 'r', 't', 'h', 'u', 'h', 'n', 'e', 's', 'n', 's', 'a', 'n', 'e', 'o', 'h', 'u', 'e', 'o', 't', 'n', 'e', 'a', 'n', 'l', 'a', 'u', 't', 'h', 'e', 'u', 'e', 'n', 't', 'e', 'u', 's', 'l', 'i', 'e', 'o', 'u', 'h', 'h', 'r', 'h', 'r', 'e', 'n', 'n', 'i', 'r', 'u', 'h']\n",
      "Test accuracy is 20.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['s', 'e', 'c', 'o', 'n', 'd', 's', 'e', 'n', 'a', 't', 'o', 'r', 'n', 'o', 'b', 'l', 'e', 'a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 't', 'a', 'k', 'e', 'y', 'o', 'u', 'r', 'c', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n', ';', 'h', 'i', 'e', 'y', 'o', 'u', 't', 'o', 'y', 'o', 'u', 'r', 'b', 'a', 'n', 'd', 's', 'l', 'e', 't', 'u', 's', 'a', 'l', 'o', 'n', 'e', 't', 'o', 'g', 'u', 'a', 'r', 'd', 'c', 'o', 'r', 'i', 'o', 'l', 'i', 'i', 'f', 't', 'h', 'e', 'y', 's', 'e', 't', 'd', 'o', 'w', 'n', 'b', 'e', 'f', 'o', 'r', 'e', \"'\", 's', 'f', 'o', 'r', 't', 'h', 'e', 'r', 'e', 'm', 'o', 'v', 'e', 'b', 'r', 'i', 'n', 'g', 'y', 'o', 'u', 'r', 'a', 'r', 'm', 'y', ';', 'b', 'u', 't', 'i', 't', 'h', 'i', 'n', 'k', 'y', 'o', 'u', \"'\", 'l', 'l', 'f', 'i', 'n', 'd', 't', 'h', 'e', 'y', \"'\", 'v', 'e', 'n', 'o', 't', 'p', 'r', 'e', 'p', 'a', 'r', 'e', 'd', 'f', 'o', 'r', 'u', 's']\n",
      "\n",
      "Target sequence:\n",
      "['e', 'c', 'o', 'n', 'd', 's', 'e', 'n', 'a', 't', 'o', 'r', 'n', 'o', 'b', 'l', 'e', 'a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 't', 'a', 'k', 'e', 'y', 'o', 'u', 'r', 'c', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n', ';', 'h', 'i', 'e', 'y', 'o', 'u', 't', 'o', 'y', 'o', 'u', 'r', 'b', 'a', 'n', 'd', 's', 'l', 'e', 't', 'u', 's', 'a', 'l', 'o', 'n', 'e', 't', 'o', 'g', 'u', 'a', 'r', 'd', 'c', 'o', 'r', 'i', 'o', 'l', 'i', 'i', 'f', 't', 'h', 'e', 'y', 's', 'e', 't', 'd', 'o', 'w', 'n', 'b', 'e', 'f', 'o', 'r', 'e', \"'\", 's', 'f', 'o', 'r', 't', 'h', 'e', 'r', 'e', 'm', 'o', 'v', 'e', 'b', 'r', 'i', 'n', 'g', 'y', 'o', 'u', 'r', 'a', 'r', 'm', 'y', ';', 'b', 'u', 't', 'i', 't', 'h', 'i', 'n', 'k', 'y', 'o', 'u', \"'\", 'l', 'l', 'f', 'i', 'n', 'd', 't', 'h', 'e', 'y', \"'\", 'v', 'e', 'n', 'o', 't', 'p', 'r', 'e', 'p', 'a', 'r', 'e', 'd', 'f', 'o', 'r', 'u', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['t', 'r', 'i', 'u', 'd', 'e', 't', 'r', 'o', 't', 'h', 'u', 'e', 'e', 'u', 'e', 'l', 'a', 'n', 's', 'i', 'r', 's', 't', 's', 't', 'h', 't', 'e', 's', 'o', 'u', 's', 's', 'i', 'u', 'e', 'e', 'n', 'e', 't', 't', 'u', 't', 't', 'e', 't', 'n', 'o', 'u', 's', 'h', 'u', 'o', 'u', 's', 'e', 'e', 't', 'd', 'e', 't', 'l', 'a', 'h', 's', 't', 'n', 'l', 'u', 'd', 'n', 'h', 'u', 'e', 's', 't', 's', 'i', 'i', 'u', 'e', 'n', 'u', 'l', 'n', 'n', 'i', 'h', 'e', 't', 'o', 'u', 'r', 'h', 'e', 'u', 'h', 'd', 'e', 'n', 'i', 'r', 'e', 'n', 's', 't', 'i', 'r', 'e', 'h', 'e', 's', 'e', 'a', 'e', 'u', 'e', 'n', 'e', 'e', 't', 's', 'e', 'o', 'u', 's', 'e', 't', 'e', 'e', 'o', 'u', 's', 's', 'h', 'n', 'h', 'e', 't', 's', 'e', 'o', 'u', 's', 't', 'l', 'l', 'i', 'r', 's', 't', 'h', 'e', 's', 'o', 't', 'e', 'n', 't', 'u', 'h', 'e', 'e', 'a', 'e', 't', 'e', 's', 'e', 'i', 'r', 'e', 's', 't']\n",
      "Test accuracy is 24.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 'o', 'd', 'o', 'u', 'b', 't', 'n', 'o', 't', 't', 'h', 'a', 't', ';', 'i', 's', 'p', 'e', 'a', 'k', 'f', 'r', 'o', 'm', 'c', 'e', 'r', 't', 'a', 'i', 'n', 't', 'i', 'e', 's']\n",
      "\n",
      "Target sequence:\n",
      "['u', 'f', 'i', 'd', 'i', 'u', 's', 'o', 'd', 'o', 'u', 'b', 't', 'n', 'o', 't', 't', 'h', 'a', 't', ';', 'i', 's', 'p', 'e', 'a', 'k', 'f', 'r', 'o', 'm', 'c', 'e', 'r', 't', 'a', 'i', 'n', 't', 'i', 'e', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['r', 's', 'i', 'r', 's', 't', 's', 't', 'u', 'e', 'u', 's', 'e', 'h', 'e', 'u', 'h', 'h', 'e', 't', 'h', 'r', 'n', 't', 'r', 'a', 'r', 'e', 'o', 'e', 'u', 'a', 'i', 'n', 'e', 'h', 'n', 't', 'd', 'h', 'n', 'n', 't']\n",
      "Test accuracy is 14.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['n', 'a', 'y', 'm', 'o', 'r', 'e', 's', 'o', 'm', 'e', 'p', 'a', 'r', 'c', 'e', 'l', 's', 'o', 'f', 't', 'h', 'e', 'i', 'r', 'p', 'o', 'w', 'e', 'r', 'a', 'r', 'e', 'f', 'o', 'r', 't', 'h', 'a', 'l', 'r', 'e', 'a', 'd', 'y', 'a', 'n', 'd', 'o', 'n', 'l', 'y', 'h', 'i', 't', 'h', 'e', 'r', 'w', 'a', 'r', 'd']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'y', 'm', 'o', 'r', 'e', 's', 'o', 'm', 'e', 'p', 'a', 'r', 'c', 'e', 'l', 's', 'o', 'f', 't', 'h', 'e', 'i', 'r', 'p', 'o', 'w', 'e', 'r', 'a', 'r', 'e', 'f', 'o', 'r', 't', 'h', 'a', 'l', 'r', 'e', 'a', 'd', 'y', 'a', 'n', 'd', 'o', 'n', 'l', 'y', 'h', 'i', 't', 'h', 'e', 'r', 'w', 'a', 'r', 'd', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['o', 't', 'o', 'u', 'u', 'e', 'n', 't', 'u', 'e', 'n', 'e', 't', 'e', 'i', 'n', 'l', 't', 'u', 'i', 'h', 'e', 't', 'n', 's', 'e', 'u', 'h', 'r', 'e', 't', 'e', 's', 'i', 'r', 'e', 'h', 'e', 't', 'l', 'e', 'a', 't', 'e', 'o', 'u', 'd', 'e', 'u', 't', 'l', 'o', 'e', 't', 'h', 'e', 's', 'e', 'h', 'r', 'e', 'e']\n",
      "Test accuracy is 24.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['i', 'l', 'e', 'a', 'v', 'e', 'y', 'o', 'u', 'r', 'h', 'o', 'n', 'o', 'u', 'r', 's']\n",
      "\n",
      "Target sequence:\n",
      "['l', 'e', 'a', 'v', 'e', 'y', 'o', 'u', 'r', 'h', 'o', 'n', 'o', 'u', 'r', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['r', 'l', 'n', 'n', 'e', 'n', 'o', 'u', 's', 's', 'e', 'u', 'd', 'u', 's', 's', 't']\n",
      "Test accuracy is 29.0%.\n",
      "\n",
      "\n",
      "Input sequence:\n",
      "['i', 'f', 'w', 'e', 'a', 'n', 'd', 'c', 'a', 'i', 'u', 's', 'm', 'a', 'r', 'c', 'i', 'u', 's', 'c', 'h', 'a', 'n', 'c', 'e', 't', 'o', 'm', 'e', 'e', 't', \"'\", 't', 'i', 's', 's', 'w', 'o', 'r', 'n', 'b', 'e', 't', 'w', 'e', 'e', 'n', 'u', 's', 'w', 'e', 's', 'h', 'a', 'l', 'l', 'e', 'v', 'e', 'r', 's', 't', 'r', 'i', 'k', 'e', 't', 'i', 'l', 'l', 'o', 'n', 'e', 'c', 'a', 'n', 'd', 'o', 'n', 'o', 'm', 'o', 'r', 'e']\n",
      "\n",
      "Target sequence:\n",
      "['f', 'w', 'e', 'a', 'n', 'd', 'c', 'a', 'i', 'u', 's', 'm', 'a', 'r', 'c', 'i', 'u', 's', 'c', 'h', 'a', 'n', 'c', 'e', 't', 'o', 'm', 'e', 'e', 't', \"'\", 't', 'i', 's', 's', 'w', 'o', 'r', 'n', 'b', 'e', 't', 'w', 'e', 'e', 'n', 'u', 's', 'w', 'e', 's', 'h', 'a', 'l', 'l', 'e', 'v', 'e', 'r', 's', 't', 'r', 'i', 'k', 'e', 't', 'i', 'l', 'l', 'o', 'n', 'e', 'c', 'a', 'n', 'd', 'o', 'n', 'o', 'm', 'o', 'r', 'e', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['r', 'i', 'r', 'r', 't', 'd', 't', 'i', 't', 'n', 's', 't', 'a', 'r', 'e', 'i', 'u', 's', 't', 'i', 'e', 't', 'e', 'i', 'n', 'h', 'u', 'e', 'n', 'r', 'h', 's', 'h', 't', 't', 't', 'e', 'u', 'e', 'e', 'e', 'n', 'h', 'h', 'r', 'n', 'd', 's', 't', 'e', 'r', 't', 'e', 't', 'l', 'l', 'n', 'e', 'n', 'e', 't', 'h', 'e', 't', 'e', 's', 'h', 't', 'l', 'l', 'u', 'e', 'n', 'i', 't', 'd', 'e', 'u', 't', 'u', 'a', 'u', 'e', 'n']\n",
      "Test accuracy is 24.0%.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Testing your network with whole test set\n",
    "\n",
    "# Load best parameters using pickle\n",
    "with open('best_parameters_rnn_shakespeare.pkl', 'rb') as f:\n",
    "    parameters = pickle.load(f)\n",
    "\n",
    "# For each sentence in test set\n",
    "for inputs, targets in test_set:\n",
    "\n",
    "    # One-hot encode input and target sequence\n",
    "    test_input = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "    test_target = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "    # Initialize hidden state as zeros\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Forward pass\n",
    "    # TODO:\n",
    "    outputs, _ = forward_pass(test_input, hidden_state, parameters)\n",
    "\n",
    "    print('Input sequence:')\n",
    "    print(inputs)\n",
    "\n",
    "    print('\\nTarget sequence:')\n",
    "    print(targets)\n",
    "\n",
    "    preds = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "    print('\\nPredicted sequence:')\n",
    "    print(preds)\n",
    "\n",
    "    accuracy = 0\n",
    "    for target, pred in zip(targets, preds):\n",
    "        accuracy += target == pred\n",
    "    accuracy /= len(targets)/100\n",
    "    \n",
    "    print(f\"Test accuracy is {np.round(accuracy)}%.\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most famous challenges for RNNs is [the vanishing gradient problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem), where the gradient, i.e. the signal to update our weights, goes to zero due to repetetive multiplications of values <1 in the backprop (or goes to oo due to repetetive multiplications of values >1 in what's called exploding gradient). This can happen to all networks, but RNNs are far more vulnurable to it due to the long sequences of backprops through time. In this section of the lab you'll see how this issue can be addressed, and \n",
    "\n",
    "Please read through [Christopher Olah's walk through](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) to understand this issue and its solutions in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ![Vanilla RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "![Terminology](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)\n",
    "\n",
    "The image above represents a normal \"vanilla\" RNN, with the yellow \"tanh\" rectangle symbolising our current network. $h_t$ represents the output in this image, so as you can see there's the x->hidden state connection, hidden state update connection, and hidden state->output connection. \n",
    "\n",
    "To amend the issue of vanishing gradients, gated hidden units were introduced, which act like a highway between states to facilitate modelling long-range dependencies. Two famous solutions exist, one called \"long short-term memory\" (LSTM) and the other \"gated recurrent unit\" (GRU). Today, we're going to look a bit closer at the LSTM (don't worry though, you're using pytorch to implement it this time).\n",
    "\n",
    "The image below, fully explained in the walk through, shows the schematic of an LSTM cell. The fundamentals from our vanilla RNN are still there, namely the tanh of our input and previous hidden state, but we now have a \"cell state\", which acts as the aforementioned highway, and three \"gates\" which update this cell state.\n",
    "\n",
    "The three gates are the input gate $i$, the forget gate $f$, and the output gate $o$. They are defined as \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "i = \\sigma ( W^i [h_{t-1}, x_t])\\\\\n",
    "f = \\sigma ( W^f [h_{t-1},x_t])\\\\\n",
    "o = \\sigma ( W^o [h_{t-1},x_t])\n",
    "\\end{aligned}\n",
    "\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $W^i$, $W^f$, and $W^o$ are the weight matrices applied to $h_{t-1},x_t$.\n",
    "\n",
    "![LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Read the walk through above and explain the function of the three gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:\n",
    "- forget gate:\n",
    "- input gate:\n",
    "- output gate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an LSTM with Pytorch\n",
    "Now for your final task. Having learned how LSTMs work in theory, it's now time to put this knowledge into practice.\n",
    "\n",
    "Using the [LSTM pytorch documentations](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), build and train an LSTM network and compare it to your self-made RNN. You won't need to import any more packages than those already imported, but you're free to use any approach as long as you can defend it during the presentation.\n",
    "\n",
    "**Exercise:** Build the LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 25):\n",
    "        super(LSTM, self).__init__()\n",
    "        num_layers = 1\n",
    "        # Recurrent layer\n",
    "        # TODO:\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                         hidden_size=hidden_size,\n",
    "                         num_layers=num_layers,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=hidden_size,\n",
    "                            out_features=input_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        # TODO:\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        # TODO:\n",
    "        x = self.l_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(36, 25)\n",
      "  (l_out): Linear(in_features=25, out_features=36, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise our network\n",
    "model = LSTM(36)\n",
    "print(model)\n",
    "min_LSTM_loss = 10000\n",
    "\n",
    "# Store training and validation loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Define a loss function and optimizer for this problem\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Had to change to lr to improve accuracy.. Descresing it didnot imporve the accuracy... Increasing it to 3e-2 and loss decreased and accuracy improved\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\n",
    "# A way to get learning rate decay\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the Shakespeare dataset: ['f', 'i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k', 'EOS']\n",
      "We have 160 samples in the training set.\n",
      "We have 20 samples in the validation set.\n",
      "We have 20 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "# Set up training, validation and test set, similar to self made RNN (toy sequence)\n",
    "#sequences, char_to_idx, idx_to_char, num_sequences, vocab_size = set_up_sequences('toy')\n",
    "#training_set, validation_set, test_set = set_up_datasets(sequences)\n",
    "\n",
    "# Set up training, validation and test set (Shakespeare)\n",
    "sequences, char_to_idx, idx_to_char, num_sequences, vocab_size = set_up_sequences()\n",
    "\n",
    "#num_sequences = len(sequences)\n",
    "#vocab_size = len(np.unique(sequences))\n",
    "print(f'A sentence from the Shakespeare dataset: {sequences[0]}')\n",
    "\n",
    "# Whole dataset is too big to effectively train on, so let's start by grabbing the first 200 sequences\n",
    "sequences = sequences[0:200]\n",
    "training_set, validation_set, test_set = set_up_datasets(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "**Exercise:** Finish the training code until you have at least 25% accuracy. If you're willing to train for a long time, feel free to use more data and bigger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 2.760613113641739, validation loss: 3.595840096473694\n",
      "Epoch 10, training loss: 2.1807689048349856, validation loss: 2.394413506984711\n",
      "Epoch 20, training loss: 2.178148590028286, validation loss: 2.3936082482337953\n",
      "Epoch 30, training loss: 2.178140301257372, validation loss: 2.3936060965061188\n",
      "Epoch 40, training loss: 2.178140319138765, validation loss: 2.3936060965061188\n",
      "Epoch 50, training loss: 2.178140319138765, validation loss: 2.3936060965061188\n",
      "Epoch 60, training loss: 2.178140319138765, validation loss: 2.3936060965061188\n",
      "Epoch 70, training loss: 2.178140319138765, validation loss: 2.3936060965061188\n",
      "\n",
      "Input sequence:\n",
      "['f', 'i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['i', 'r', 's', 't', 'h', 'i', 't', 'i', 'z', 'e', 'n', 'i', 'e', 'e', 'o', 'r', 'e', 's', 'e', 'l', 'r', 'e', 'w', 'h', 'n', 'n', 'i', 'n', 'd', 'o', 'a', 'r', 's', 'h', 'e', 'r', 'e', 'a', 'r', 'r', 'e', 'a', 'r', 'e', 'o', 'r', 'k', 'e']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "# had to increase the epochs from 25 to 80 to improve accuracy\n",
    "num_epochs = 80\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_idx = [char_to_idx[char] for char in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # TODO:\n",
    "        outputs = model.forward(inputs_one_hot)\n",
    "        \n",
    "        # Compute loss\n",
    "        # TODO:\n",
    "        loss = criterion(outputs, targets_idx)\n",
    "\n",
    "        if loss<min_LSTM_loss:\n",
    "            #with open('best_model.pt', 'wb') as f:\n",
    "            #    torch.save(model, f)\n",
    "            torch.save(model, 'best_model.pt')\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_idx = [char_to_idx[char] for char in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # TODO:\n",
    "        outputs = model.forward(inputs_one_hot)\n",
    "        \n",
    "        # Compute loss\n",
    "        # TODO:\n",
    "        loss = criterion(outputs, targets_idx)\n",
    "\n",
    "        \n",
    "        # Reset gradients\n",
    "        # TODO:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute gradients\n",
    "        # TODO:\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        # TODO:\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate (advanced technique, can be ignored)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 5 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "\n",
    "# Get first sentence in training set\n",
    "inputs, targets = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "targets_idx = [char_to_idx[char] for char in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx)\n",
    "\n",
    "# Forward pass\n",
    "# TODO:\n",
    "best_model = LSTM(36)\n",
    "best_model = torch.load('best_model.pt')\n",
    "outputs = best_model.forward(inputs_one_hot).data.numpy()\n",
    "\n",
    "print('\\nInput sequence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "preds = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "print('\\nPredicted sequence:')\n",
    "print(preds)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence:\n",
      "['s', 'o', 'n', 'o', 'w', 't', 'h', 'e', 'g', 'a', 't', 'e', 's', 'a', 'r', 'e', 'o', 'p', 'e', 'n', 'o', 'w', 'p', 'r', 'o', 'v', 'e', 'g', 'o', 'o', 'd', 's', 'e', 'c', 'o', 'n', 'd', 's', \"'\", 't', 'i', 's', 'f', 'o', 'r', 't', 'h', 'e', 'f', 'o', 'l', 'l', 'o', 'w', 'e', 'r', 's', 'f', 'o', 'r', 't', 'u', 'n', 'e', 'w', 'i', 'd', 'e', 'n', 's', 't', 'h', 'e', 'm', 'n', 'o', 't', 'f', 'o', 'r', 't', 'h', 'e', 'f', 'l', 'i', 'e', 'r', 's', 'm', 'a', 'r', 'k', 'm', 'e', 'a', 'n', 'd', 'd', 'o', 't', 'h', 'e', 'l', 'i', 'k', 'e']\n",
      "\n",
      "Target sequence:\n",
      "['o', 'n', 'o', 'w', 't', 'h', 'e', 'g', 'a', 't', 'e', 's', 'a', 'r', 'e', 'o', 'p', 'e', 'n', 'o', 'w', 'p', 'r', 'o', 'v', 'e', 'g', 'o', 'o', 'd', 's', 'e', 'c', 'o', 'n', 'd', 's', \"'\", 't', 'i', 's', 'f', 'o', 'r', 't', 'h', 'e', 'f', 'o', 'l', 'l', 'o', 'w', 'e', 'r', 's', 'f', 'o', 'r', 't', 'u', 'n', 'e', 'w', 'i', 'd', 'e', 'n', 's', 't', 'h', 'e', 'm', 'n', 'o', 't', 'f', 'o', 'r', 't', 'h', 'e', 'f', 'l', 'i', 'e', 'r', 's', 'm', 'a', 'r', 'k', 'm', 'e', 'a', 'n', 'd', 'd', 'o', 't', 'h', 'e', 'l', 'i', 'k', 'e', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 36.0%.\n",
      "\n",
      "Input sequence:\n",
      "['f', 'i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'f', 'o', 'o', 'l', '-', 'h', 'a', 'r', 'd', 'i', 'n', 'e', 's', 's', ';', 'n', 'o', 't', 'i']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'f', 'o', 'o', 'l', '-', 'h', 'a', 'r', 'd', 'i', 'n', 'e', 's', 's', ';', 'n', 'o', 't', 'i', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 10.0%.\n",
      "\n",
      "Input sequence:\n",
      "['s', 'e', 'c', 'o', 'n', 'd', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'n', 'o', 'r', 'i']\n",
      "\n",
      "Target sequence:\n",
      "['e', 'c', 'o', 'n', 'd', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'n', 'o', 'r', 'i', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 6.0%.\n",
      "\n",
      "Input sequence:\n",
      "['f', 'i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 's', 'e', 'e', 't', 'h', 'e', 'y', 'h', 'a', 'v', 'e', 's', 'h', 'u', 't', 'h', 'i', 'm', 'i', 'n']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 's', 'e', 'e', 't', 'h', 'e', 'y', 'h', 'a', 'v', 'e', 's', 'h', 'u', 't', 'h', 'i', 'm', 'i', 'n', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 3.0%.\n",
      "\n",
      "Input sequence:\n",
      "['a', 'l', 'l', 't', 'o', 't', 'h', 'e', 'p', 'o', 't', 'i', 'w', 'a', 'r', 'r', 'a', 'n', 't', 'h', 'i', 'm']\n",
      "\n",
      "Target sequence:\n",
      "['l', 'l', 't', 'o', 't', 'h', 'e', 'p', 'o', 't', 'i', 'w', 'a', 'r', 'r', 'a', 'n', 't', 'h', 'i', 'm', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 23.0%.\n",
      "\n",
      "Input sequence:\n",
      "['l', 'a', 'r', 't', 'i', 'u', 's', 'w', 'h', 'a', 't', 'i', 's', 'b', 'e', 'c', 'o', 'm', 'e', 'o', 'f', 'm', 'a', 'r', 'c', 'i', 'u', 's', '?', 'a', 'l', 'l', 's', 'l', 'a', 'i', 'n', 's', 'i', 'r', 'd', 'o', 'u', 'b', 't', 'l', 'e', 's', 's']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 't', 'i', 'u', 's', 'w', 'h', 'a', 't', 'i', 's', 'b', 'e', 'c', 'o', 'm', 'e', 'o', 'f', 'm', 'a', 'r', 'c', 'i', 'u', 's', '?', 'a', 'l', 'l', 's', 'l', 'a', 'i', 'n', 's', 'i', 'r', 'd', 'o', 'u', 'b', 't', 'l', 'e', 's', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 6.0%.\n",
      "\n",
      "Input sequence:\n",
      "['f', 'i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'f', 'o', 'l', 'l', 'o', 'w', 'i', 'n', 'g', 't', 'h', 'e', 'f', 'l', 'i', 'e', 'r', 's', 'a', 't', 't', 'h', 'e', 'v', 'e', 'r', 'y', 'h', 'e', 'e', 'l', 's', 'w', 'i', 't', 'h', 't', 'h', 'e', 'm', 'h', 'e', 'e', 'n', 't', 'e', 'r', 's', ';', 'w', 'h', 'o', 'u', 'p', 'o', 'n', 't', 'h', 'e', 's', 'u', 'd', 'd', 'e', 'n', 'c', 'l', 'a', 'p', 'p', \"'\", 'd', 't', 'o', 't', 'h', 'e', 'i', 'r', 'g', 'a', 't', 'e', 's', 'h', 'e', 'i', 's', 'h', 'i', 'm', 's', 'e', 'l', 'f', 'a', 'l', 'o', 'n', 'e', 't', 'o', 'a', 'n', 's', 'w', 'e', 'r', 'a', 'l', 'l', 't', 'h', 'e', 'c', 'i', 't', 'y']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'f', 'o', 'l', 'l', 'o', 'w', 'i', 'n', 'g', 't', 'h', 'e', 'f', 'l', 'i', 'e', 'r', 's', 'a', 't', 't', 'h', 'e', 'v', 'e', 'r', 'y', 'h', 'e', 'e', 'l', 's', 'w', 'i', 't', 'h', 't', 'h', 'e', 'm', 'h', 'e', 'e', 'n', 't', 'e', 'r', 's', ';', 'w', 'h', 'o', 'u', 'p', 'o', 'n', 't', 'h', 'e', 's', 'u', 'd', 'd', 'e', 'n', 'c', 'l', 'a', 'p', 'p', \"'\", 'd', 't', 'o', 't', 'h', 'e', 'i', 'r', 'g', 'a', 't', 'e', 's', 'h', 'e', 'i', 's', 'h', 'i', 'm', 's', 'e', 'l', 'f', 'a', 'l', 'o', 'n', 'e', 't', 'o', 'a', 'n', 's', 'w', 'e', 'r', 'a', 'l', 'l', 't', 'h', 'e', 'c', 'i', 't', 'y', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 7.0%.\n",
      "\n",
      "Input sequence:\n",
      "['l', 'a', 'r', 't', 'i', 'u', 's', 'o', 'n', 'o', 'b', 'l', 'e', 'f', 'e', 'l', 'l', 'o', 'w', '!', 'w', 'h', 'o', 's', 'e', 'n', 's', 'i', 'b', 'l', 'y', 'o', 'u', 't', 'd', 'a', 'r', 'e', 's', 'h', 'i', 's', 's', 'e', 'n', 's', 'e', 'l', 'e', 's', 's', 's', 'w', 'o', 'r', 'd', 'a', 'n', 'd', 'w', 'h', 'e', 'n', 'i', 't', 'b', 'o', 'w', 's', 's', 't', 'a', 'n', 'd', 's', 'u', 'p']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 't', 'i', 'u', 's', 'o', 'n', 'o', 'b', 'l', 'e', 'f', 'e', 'l', 'l', 'o', 'w', '!', 'w', 'h', 'o', 's', 'e', 'n', 's', 'i', 'b', 'l', 'y', 'o', 'u', 't', 'd', 'a', 'r', 'e', 's', 'h', 'i', 's', 's', 'e', 'n', 's', 'e', 'l', 'e', 's', 's', 's', 'w', 'o', 'r', 'd', 'a', 'n', 'd', 'w', 'h', 'e', 'n', 'i', 't', 'b', 'o', 'w', 's', 's', 't', 'a', 'n', 'd', 's', 'u', 'p', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 6.0%.\n",
      "\n",
      "Input sequence:\n",
      "['t', 'h', 'o', 'u', 'a', 'r', 't', 'l', 'e', 'f', 't', 'm', 'a', 'r', 'c', 'i', 'u', 's', 'a', 'c', 'a', 'r', 'b', 'u', 'n', 'c', 'l', 'e', 'e', 'n', 't', 'i', 'r', 'e', 'a', 's', 'b', 'i', 'g', 'a', 's', 't', 'h', 'o', 'u', 'a', 'r', 't', 'w', 'e', 'r', 'e', 'n', 'o', 't', 's', 'o', 'r', 'i', 'c', 'h', 'a', 'j', 'e', 'w', 'e', 'l']\n",
      "\n",
      "Target sequence:\n",
      "['h', 'o', 'u', 'a', 'r', 't', 'l', 'e', 'f', 't', 'm', 'a', 'r', 'c', 'i', 'u', 's', 'a', 'c', 'a', 'r', 'b', 'u', 'n', 'c', 'l', 'e', 'e', 'n', 't', 'i', 'r', 'e', 'a', 's', 'b', 'i', 'g', 'a', 's', 't', 'h', 'o', 'u', 'a', 'r', 't', 'w', 'e', 'r', 'e', 'n', 'o', 't', 's', 'o', 'r', 'i', 'c', 'h', 'a', 'j', 'e', 'w', 'e', 'l', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 3.0%.\n",
      "\n",
      "Input sequence:\n",
      "['t', 'h', 'o', 'u', 'w', 'a', 's', 't', 'a', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'e', 'v', 'e', 'n', 't', 'o', 'c', 'a', 't', 'o', \"'\", 's', 'w', 'i', 's', 'h', 'n', 'o', 't', 'f', 'i', 'e', 'r', 'c', 'e', 'a', 'n', 'd', 't', 'e', 'r', 'r', 'i', 'b', 'l', 'e', 'o', 'n', 'l', 'y', 'i', 'n', 's', 't', 'r', 'o', 'k', 'e', 's', ';', 'b', 'u', 't', 'w', 'i', 't', 'h', 't', 'h', 'y', 'g', 'r', 'i', 'm', 'l', 'o', 'o', 'k', 's', 'a', 'n', 'd', 't', 'h', 'e', 't', 'h', 'u', 'n', 'd', 'e', 'r', '-', 'l', 'i', 'k', 'e', 'p', 'e', 'r', 'c', 'u', 's', 's', 'i', 'o', 'n', 'o', 'f', 't', 'h', 'y', 's', 'o', 'u', 'n', 'd', 's', 't', 'h', 'o', 'u', 'm', 'a', 'd', 's', 't', 't', 'h', 'i', 'n', 'e', 'e', 'n', 'e', 'm', 'i', 'e', 's', 's', 'h', 'a', 'k', 'e', 'a', 's', 'i', 'f', 't', 'h', 'e', 'w', 'o', 'r', 'l', 'd', 'w', 'e', 'r', 'e', 'f', 'e', 'v', 'e', 'r', 'o', 'u', 's', 'a', 'n', 'd', 'd', 'i', 'd', 't', 'r', 'e', 'm', 'b', 'l', 'e']\n",
      "\n",
      "Target sequence:\n",
      "['h', 'o', 'u', 'w', 'a', 's', 't', 'a', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'e', 'v', 'e', 'n', 't', 'o', 'c', 'a', 't', 'o', \"'\", 's', 'w', 'i', 's', 'h', 'n', 'o', 't', 'f', 'i', 'e', 'r', 'c', 'e', 'a', 'n', 'd', 't', 'e', 'r', 'r', 'i', 'b', 'l', 'e', 'o', 'n', 'l', 'y', 'i', 'n', 's', 't', 'r', 'o', 'k', 'e', 's', ';', 'b', 'u', 't', 'w', 'i', 't', 'h', 't', 'h', 'y', 'g', 'r', 'i', 'm', 'l', 'o', 'o', 'k', 's', 'a', 'n', 'd', 't', 'h', 'e', 't', 'h', 'u', 'n', 'd', 'e', 'r', '-', 'l', 'i', 'k', 'e', 'p', 'e', 'r', 'c', 'u', 's', 's', 'i', 'o', 'n', 'o', 'f', 't', 'h', 'y', 's', 'o', 'u', 'n', 'd', 's', 't', 'h', 'o', 'u', 'm', 'a', 'd', 's', 't', 't', 'h', 'i', 'n', 'e', 'e', 'n', 'e', 'm', 'i', 'e', 's', 's', 'h', 'a', 'k', 'e', 'a', 's', 'i', 'f', 't', 'h', 'e', 'w', 'o', 'r', 'l', 'd', 'w', 'e', 'r', 'e', 'f', 'e', 'v', 'e', 'r', 'o', 'u', 's', 'a', 'n', 'd', 'd', 'i', 'd', 't', 'r', 'e', 'm', 'b', 'l', 'e', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 2.0%.\n",
      "\n",
      "Input sequence:\n",
      "['f', 'i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'l', 'o', 'o', 'k', 's', 'i', 'r']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 's', 'o', 'l', 'd', 'i', 'e', 'r', 'l', 'o', 'o', 'k', 's', 'i', 'r', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 16.0%.\n",
      "\n",
      "Input sequence:\n",
      "['l', 'a', 'r', 't', 'i', 'u', 's', 'o', \"'\", 't', 'i', 's', 'm', 'a', 'r', 'c', 'i', 'u', 's', '!', 'l', 'e', 't', \"'\", 's', 'f', 'e', 't', 'c', 'h', 'h', 'i', 'm', 'o', 'f', 'f', 'o', 'r', 'm', 'a', 'k', 'e', 'r', 'e', 'm', 'a', 'i', 'n', 'a', 'l', 'i', 'k', 'e']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 't', 'i', 'u', 's', 'o', \"'\", 't', 'i', 's', 'm', 'a', 'r', 'c', 'i', 'u', 's', '!', 'l', 'e', 't', \"'\", 's', 'f', 'e', 't', 'c', 'h', 'h', 'i', 'm', 'o', 'f', 'f', 'o', 'r', 'm', 'a', 'k', 'e', 'r', 'e', 'm', 'a', 'i', 'n', 'a', 'l', 'i', 'k', 'e', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 4.0%.\n",
      "\n",
      "Input sequence:\n",
      "['f', 'i', 'r', 's', 't', 'r', 'o', 'm', 'a', 'n', 't', 'h', 'i', 's', 'w', 'i', 'l', 'l', 'i', 'c', 'a', 'r', 'r', 'y', 't', 'o', 'r', 'o', 'm', 'e']\n",
      "\n",
      "Target sequence:\n",
      "['i', 'r', 's', 't', 'r', 'o', 'm', 'a', 'n', 't', 'h', 'i', 's', 'w', 'i', 'l', 'l', 'i', 'c', 'a', 'r', 'r', 'y', 't', 'o', 'r', 'o', 'm', 'e', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 3.0%.\n",
      "\n",
      "Input sequence:\n",
      "['s', 'e', 'c', 'o', 'n', 'd', 'r', 'o', 'm', 'a', 'n', 'a', 'n', 'd', 'i', 't', 'h', 'i', 's']\n",
      "\n",
      "Target sequence:\n",
      "['e', 'c', 'o', 'n', 'd', 'r', 'o', 'm', 'a', 'n', 'a', 'n', 'd', 'i', 't', 'h', 'i', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 16.0%.\n",
      "\n",
      "Input sequence:\n",
      "['t', 'h', 'i', 'r', 'd', 'r', 'o', 'm', 'a', 'n', 'a', 'm', 'u', 'r', 'r', 'a', 'i', 'n', 'o', 'n', \"'\", 't', '!', 'i', 't', 'o', 'o', 'k', 't', 'h', 'i', 's', 'f', 'o', 'r', 's', 'i', 'l', 'v', 'e', 'r']\n",
      "\n",
      "Target sequence:\n",
      "['h', 'i', 'r', 'd', 'r', 'o', 'm', 'a', 'n', 'a', 'm', 'u', 'r', 'r', 'a', 'i', 'n', 'o', 'n', \"'\", 't', '!', 'i', 't', 'o', 'o', 'k', 't', 'h', 'i', 's', 'f', 'o', 'r', 's', 'i', 'l', 'v', 'e', 'r', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 7.0%.\n",
      "\n",
      "Input sequence:\n",
      "['m', 'a', 'r', 'c', 'i', 'u', 's', 's', 'e', 'e', 'h', 'e', 'r', 'e', 't', 'h', 'e', 's', 'e', 'm', 'o', 'v', 'e', 'r', 's', 't', 'h', 'a', 't', 'd', 'o', 'p', 'r', 'i', 'z', 'e', 't', 'h', 'e', 'i', 'r', 'h', 'o', 'u', 'r', 's', 'a', 't', 'a', 'c', 'r', 'a', 'c', 'k', \"'\", 'd', 'd', 'r', 'a', 'c', 'h', 'm', '!', 'c', 'u', 's', 'h', 'i', 'o', 'n', 's', 'l', 'e', 'a', 'd', 'e', 'n', 's', 'p', 'o', 'o', 'n', 's', 'i', 'r', 'o', 'n', 's', 'o', 'f', 'a', 'd', 'o', 'i', 't', 'd', 'o', 'u', 'b', 'l', 'e', 't', 's', 't', 'h', 'a', 't', 'h', 'a', 'n', 'g', 'm', 'e', 'n', 'w', 'o', 'u', 'l', 'd', 'b', 'u', 'r', 'y', 'w', 'i', 't', 'h', 't', 'h', 'o', 's', 'e', 't', 'h', 'a', 't', 'w', 'o', 'r', 'e', 't', 'h', 'e', 'm', 't', 'h', 'e', 's', 'e', 'b', 'a', 's', 'e', 's', 'l', 'a', 'v', 'e', 's', 'e', 'r', 'e', 'y', 'e', 't', 't', 'h', 'e', 'f', 'i', 'g', 'h', 't', 'b', 'e', 'd', 'o', 'n', 'e', 'p', 'a', 'c', 'k', 'u', 'p', 'd', 'o', 'w', 'n', 'w', 'i', 't', 'h', 't', 'h', 'e', 'm', '!', 'a', 'n', 'd', 'h', 'a', 'r', 'k', 'w', 'h', 'a', 't', 'n', 'o', 'i', 's', 'e', 't', 'h', 'e', 'g', 'e', 'n', 'e', 'r', 'a', 'l', 'm', 'a', 'k', 'e', 's', '!', 't', 'o', 'h', 'i', 'm', '!', 't', 'h', 'e', 'r', 'e', 'i', 's', 't', 'h', 'e', 'm', 'a', 'n', 'o', 'f', 'm', 'y', 's', 'o', 'u', 'l', \"'\", 's', 'h', 'a', 't', 'e', 'a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 'p', 'i', 'e', 'r', 'c', 'i', 'n', 'g', 'o', 'u', 'r', 'r', 'o', 'm', 'a', 'n', 's', 't', 'h', 'e', 'n', 'v', 'a', 'l', 'i', 'a', 'n', 't', 't', 'i', 't', 'u', 's', 't', 'a', 'k', 'e', 'c', 'o', 'n', 'v', 'e', 'n', 'i', 'e', 'n', 't', 'n', 'u', 'm', 'b', 'e', 'r', 's', 't', 'o', 'm', 'a', 'k', 'e', 'g', 'o', 'o', 'd', 't', 'h', 'e', 'c', 'i', 't', 'y', ';', 'w', 'h', 'i', 'l', 's', 't', 'i', 'w', 'i', 't', 'h', 't', 'h', 'o', 's', 'e', 't', 'h', 'a', 't', 'h', 'a', 'v', 'e', 't', 'h', 'e', 's', 'p', 'i', 'r', 'i', 't', 'w', 'i', 'l', 'l', 'h', 'a', 's', 't', 'e', 't', 'o', 'h', 'e', 'l', 'p', 'c', 'o', 'm', 'i', 'n', 'i', 'u', 's']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 'c', 'i', 'u', 's', 's', 'e', 'e', 'h', 'e', 'r', 'e', 't', 'h', 'e', 's', 'e', 'm', 'o', 'v', 'e', 'r', 's', 't', 'h', 'a', 't', 'd', 'o', 'p', 'r', 'i', 'z', 'e', 't', 'h', 'e', 'i', 'r', 'h', 'o', 'u', 'r', 's', 'a', 't', 'a', 'c', 'r', 'a', 'c', 'k', \"'\", 'd', 'd', 'r', 'a', 'c', 'h', 'm', '!', 'c', 'u', 's', 'h', 'i', 'o', 'n', 's', 'l', 'e', 'a', 'd', 'e', 'n', 's', 'p', 'o', 'o', 'n', 's', 'i', 'r', 'o', 'n', 's', 'o', 'f', 'a', 'd', 'o', 'i', 't', 'd', 'o', 'u', 'b', 'l', 'e', 't', 's', 't', 'h', 'a', 't', 'h', 'a', 'n', 'g', 'm', 'e', 'n', 'w', 'o', 'u', 'l', 'd', 'b', 'u', 'r', 'y', 'w', 'i', 't', 'h', 't', 'h', 'o', 's', 'e', 't', 'h', 'a', 't', 'w', 'o', 'r', 'e', 't', 'h', 'e', 'm', 't', 'h', 'e', 's', 'e', 'b', 'a', 's', 'e', 's', 'l', 'a', 'v', 'e', 's', 'e', 'r', 'e', 'y', 'e', 't', 't', 'h', 'e', 'f', 'i', 'g', 'h', 't', 'b', 'e', 'd', 'o', 'n', 'e', 'p', 'a', 'c', 'k', 'u', 'p', 'd', 'o', 'w', 'n', 'w', 'i', 't', 'h', 't', 'h', 'e', 'm', '!', 'a', 'n', 'd', 'h', 'a', 'r', 'k', 'w', 'h', 'a', 't', 'n', 'o', 'i', 's', 'e', 't', 'h', 'e', 'g', 'e', 'n', 'e', 'r', 'a', 'l', 'm', 'a', 'k', 'e', 's', '!', 't', 'o', 'h', 'i', 'm', '!', 't', 'h', 'e', 'r', 'e', 'i', 's', 't', 'h', 'e', 'm', 'a', 'n', 'o', 'f', 'm', 'y', 's', 'o', 'u', 'l', \"'\", 's', 'h', 'a', 't', 'e', 'a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 'p', 'i', 'e', 'r', 'c', 'i', 'n', 'g', 'o', 'u', 'r', 'r', 'o', 'm', 'a', 'n', 's', 't', 'h', 'e', 'n', 'v', 'a', 'l', 'i', 'a', 'n', 't', 't', 'i', 't', 'u', 's', 't', 'a', 'k', 'e', 'c', 'o', 'n', 'v', 'e', 'n', 'i', 'e', 'n', 't', 'n', 'u', 'm', 'b', 'e', 'r', 's', 't', 'o', 'm', 'a', 'k', 'e', 'g', 'o', 'o', 'd', 't', 'h', 'e', 'c', 'i', 't', 'y', ';', 'w', 'h', 'i', 'l', 's', 't', 'i', 'w', 'i', 't', 'h', 't', 'h', 'o', 's', 'e', 't', 'h', 'a', 't', 'h', 'a', 'v', 'e', 't', 'h', 'e', 's', 'p', 'i', 'r', 'i', 't', 'w', 'i', 'l', 'l', 'h', 'a', 's', 't', 'e', 't', 'o', 'h', 'e', 'l', 'p', 'c', 'o', 'm', 'i', 'n', 'i', 'u', 's', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 3.0%.\n",
      "\n",
      "Input sequence:\n",
      "['l', 'a', 'r', 't', 'i', 'u', 's', 'w', 'o', 'r', 't', 'h', 'y', 's', 'i', 'r', 't', 'h', 'o', 'u', 'b', 'l', 'e', 'e', 'd', \"'\", 's', 't', ';', 't', 'h', 'y', 'e', 'x', 'e', 'r', 'c', 'i', 's', 'e', 'h', 'a', 't', 'h', 'b', 'e', 'e', 'n', 't', 'o', 'o', 'v', 'i', 'o', 'l', 'e', 'n', 't', 'f', 'o', 'r', 'a', 's', 'e', 'c', 'o', 'n', 'd', 'c', 'o', 'u', 'r', 's', 'e', 'o', 'f', 'f', 'i', 'g', 'h', 't']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 't', 'i', 'u', 's', 'w', 'o', 'r', 't', 'h', 'y', 's', 'i', 'r', 't', 'h', 'o', 'u', 'b', 'l', 'e', 'e', 'd', \"'\", 's', 't', ';', 't', 'h', 'y', 'e', 'x', 'e', 'r', 'c', 'i', 's', 'e', 'h', 'a', 't', 'h', 'b', 'e', 'e', 'n', 't', 'o', 'o', 'v', 'i', 'o', 'l', 'e', 'n', 't', 'f', 'o', 'r', 'a', 's', 'e', 'c', 'o', 'n', 'd', 'c', 'o', 'u', 'r', 's', 'e', 'o', 'f', 'f', 'i', 'g', 'h', 't', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 12.0%.\n",
      "\n",
      "Input sequence:\n",
      "['m', 'a', 'r', 'c', 'i', 'u', 's', 's', 'i', 'r', 'p', 'r', 'a', 'i', 's', 'e', 'm', 'e', 'n', 'o', 't', ';', 'm', 'y', 'w', 'o', 'r', 'k', 'h', 'a', 't', 'h', 'y', 'e', 't', 'n', 'o', 't', 'w', 'a', 'r', 'm', \"'\", 'd', 'm', 'e', 'f', 'a', 'r', 'e', 'y', 'o', 'u', 'w', 'e', 'l', 'l', 't', 'h', 'e', 'b', 'l', 'o', 'o', 'd', 'i', 'd', 'r', 'o', 'p', 'i', 's', 'r', 'a', 't', 'h', 'e', 'r', 'p', 'h', 'y', 's', 'i', 'c', 'a', 'l', 't', 'h', 'a', 'n', 'd', 'a', 'n', 'g', 'e', 'r', 'o', 'u', 's', 't', 'o', 'm', 'e', 't', 'o', 'a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 't', 'h', 'u', 's', 'i', 'w', 'i', 'l', 'l', 'a', 'p', 'p', 'e', 'a', 'r', 'a', 'n', 'd', 'f', 'i', 'g', 'h', 't']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 'c', 'i', 'u', 's', 's', 'i', 'r', 'p', 'r', 'a', 'i', 's', 'e', 'm', 'e', 'n', 'o', 't', ';', 'm', 'y', 'w', 'o', 'r', 'k', 'h', 'a', 't', 'h', 'y', 'e', 't', 'n', 'o', 't', 'w', 'a', 'r', 'm', \"'\", 'd', 'm', 'e', 'f', 'a', 'r', 'e', 'y', 'o', 'u', 'w', 'e', 'l', 'l', 't', 'h', 'e', 'b', 'l', 'o', 'o', 'd', 'i', 'd', 'r', 'o', 'p', 'i', 's', 'r', 'a', 't', 'h', 'e', 'r', 'p', 'h', 'y', 's', 'i', 'c', 'a', 'l', 't', 'h', 'a', 'n', 'd', 'a', 'n', 'g', 'e', 'r', 'o', 'u', 's', 't', 'o', 'm', 'e', 't', 'o', 'a', 'u', 'f', 'i', 'd', 'i', 'u', 's', 't', 'h', 'u', 's', 'i', 'w', 'i', 'l', 'l', 'a', 'p', 'p', 'e', 'a', 'r', 'a', 'n', 'd', 'f', 'i', 'g', 'h', 't', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 7.0%.\n",
      "\n",
      "Input sequence:\n",
      "['l', 'a', 'r', 't', 'i', 'u', 's', 'n', 'o', 'w', 't', 'h', 'e', 'f', 'a', 'i', 'r', 'g', 'o', 'd', 'd', 'e', 's', 's', 'f', 'o', 'r', 't', 'u', 'n', 'e', 'f', 'a', 'l', 'l', 'd', 'e', 'e', 'p', 'i', 'n', 'l', 'o', 'v', 'e', 'w', 'i', 't', 'h', 't', 'h', 'e', 'e', ';', 'a', 'n', 'd', 'h', 'e', 'r', 'g', 'r', 'e', 'a', 't', 'c', 'h', 'a', 'r', 'm', 's', 'm', 'i', 's', 'g', 'u', 'i', 'd', 'e', 't', 'h', 'y', 'o', 'p', 'p', 'o', 's', 'e', 'r', 's', \"'\", 's', 'w', 'o', 'r', 'd', 's', '!', 'b', 'o', 'l', 'd', 'g', 'e', 'n', 't', 'l', 'e', 'm', 'a', 'n', 'p', 'r', 'o', 's', 'p', 'e', 'r', 'i', 't', 'y', 'b', 'e', 't', 'h', 'y', 'p', 'a', 'g', 'e', '!', 'm', 'a', 'r', 'c', 'i', 'u', 's', 't', 'h', 'y', 'f', 'r', 'i', 'e', 'n', 'd', 'n', 'o', 'l', 'e', 's', 's', 't', 'h', 'a', 'n', 't', 'h', 'o', 's', 'e', 's', 'h', 'e', 'p', 'l', 'a', 'c', 'e', 't', 'h', 'h', 'i', 'g', 'h', 'e', 's', 't', '!', 's', 'o', 'f', 'a', 'r', 'e', 'w', 'e', 'l', 'l']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 't', 'i', 'u', 's', 'n', 'o', 'w', 't', 'h', 'e', 'f', 'a', 'i', 'r', 'g', 'o', 'd', 'd', 'e', 's', 's', 'f', 'o', 'r', 't', 'u', 'n', 'e', 'f', 'a', 'l', 'l', 'd', 'e', 'e', 'p', 'i', 'n', 'l', 'o', 'v', 'e', 'w', 'i', 't', 'h', 't', 'h', 'e', 'e', ';', 'a', 'n', 'd', 'h', 'e', 'r', 'g', 'r', 'e', 'a', 't', 'c', 'h', 'a', 'r', 'm', 's', 'm', 'i', 's', 'g', 'u', 'i', 'd', 'e', 't', 'h', 'y', 'o', 'p', 'p', 'o', 's', 'e', 'r', 's', \"'\", 's', 'w', 'o', 'r', 'd', 's', '!', 'b', 'o', 'l', 'd', 'g', 'e', 'n', 't', 'l', 'e', 'm', 'a', 'n', 'p', 'r', 'o', 's', 'p', 'e', 'r', 'i', 't', 'y', 'b', 'e', 't', 'h', 'y', 'p', 'a', 'g', 'e', '!', 'm', 'a', 'r', 'c', 'i', 'u', 's', 't', 'h', 'y', 'f', 'r', 'i', 'e', 'n', 'd', 'n', 'o', 'l', 'e', 's', 's', 't', 'h', 'a', 'n', 't', 'h', 'o', 's', 'e', 's', 'h', 'e', 'p', 'l', 'a', 'c', 'e', 't', 'h', 'h', 'i', 'g', 'h', 'e', 's', 't', '!', 's', 'o', 'f', 'a', 'r', 'e', 'w', 'e', 'l', 'l', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 3.0%.\n",
      "\n",
      "Input sequence:\n",
      "['l', 'a', 'r', 't', 'i', 'u', 's', 't', 'h', 'o', 'u', 'w', 'o', 'r', 't', 'h', 'i', 'e', 's', 't', 'm', 'a', 'r', 'c', 'i', 'u', 's', '!', 'g', 'o', 's', 'o', 'u', 'n', 'd', 't', 'h', 'y', 't', 'r', 'u', 'm', 'p', 'e', 't', 'i', 'n', 't', 'h', 'e', 'm', 'a', 'r', 'k', 'e', 't', '-', 'p', 'l', 'a', 'c', 'e', ';', 'c', 'a', 'l', 'l', 't', 'h', 'i', 't', 'h', 'e', 'r', 'a', 'l', 'l', 't', 'h', 'e', 'o', 'f', 'f', 'i', 'c', 'e', 'r', 's', 'o', \"'\", 't', 'h', 'e', 't', 'o', 'w', 'n', 'w', 'h', 'e', 'r', 'e', 't', 'h', 'e', 'y', 's', 'h', 'a', 'l', 'l', 'k', 'n', 'o', 'w', 'o', 'u', 'r', 'm', 'i', 'n', 'd', 'a', 'w', 'a', 'y', '!', 'c', 'o', 'm', 'i', 'n', 'i', 'u', 's', 'b', 'r', 'e', 'a', 't', 'h', 'e', 'y', 'o', 'u', 'm', 'y', 'f', 'r', 'i', 'e', 'n', 'd', 's', 'w', 'e', 'l', 'l', 'f', 'o', 'u', 'g', 'h', 't', ';', 'w', 'e', 'a', 'r', 'e', 'c', 'o', 'm', 'e', 'o', 'f', 'f', 'l', 'i', 'k', 'e', 'r', 'o', 'm', 'a', 'n', 's', 'n', 'e', 'i', 't', 'h', 'e', 'r', 'f', 'o', 'o', 'l', 'i', 's', 'h', 'i', 'n', 'o', 'u', 'r', 's', 't', 'a', 'n', 'd', 's', 'n', 'o', 'r', 'c', 'o', 'w', 'a', 'r', 'd', 'l', 'y', 'i', 'n', 'r', 'e', 't', 'i', 'r', 'e', 'b', 'e', 'l', 'i', 'e', 'v', 'e', 'm', 'e', 's', 'i', 'r', 's', 'w', 'e', 's', 'h', 'a', 'l', 'l', 'b', 'e', 'c', 'h', 'a', 'r', 'g', 'e', 'd', 'a', 'g', 'a', 'i', 'n']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'r', 't', 'i', 'u', 's', 't', 'h', 'o', 'u', 'w', 'o', 'r', 't', 'h', 'i', 'e', 's', 't', 'm', 'a', 'r', 'c', 'i', 'u', 's', '!', 'g', 'o', 's', 'o', 'u', 'n', 'd', 't', 'h', 'y', 't', 'r', 'u', 'm', 'p', 'e', 't', 'i', 'n', 't', 'h', 'e', 'm', 'a', 'r', 'k', 'e', 't', '-', 'p', 'l', 'a', 'c', 'e', ';', 'c', 'a', 'l', 'l', 't', 'h', 'i', 't', 'h', 'e', 'r', 'a', 'l', 'l', 't', 'h', 'e', 'o', 'f', 'f', 'i', 'c', 'e', 'r', 's', 'o', \"'\", 't', 'h', 'e', 't', 'o', 'w', 'n', 'w', 'h', 'e', 'r', 'e', 't', 'h', 'e', 'y', 's', 'h', 'a', 'l', 'l', 'k', 'n', 'o', 'w', 'o', 'u', 'r', 'm', 'i', 'n', 'd', 'a', 'w', 'a', 'y', '!', 'c', 'o', 'm', 'i', 'n', 'i', 'u', 's', 'b', 'r', 'e', 'a', 't', 'h', 'e', 'y', 'o', 'u', 'm', 'y', 'f', 'r', 'i', 'e', 'n', 'd', 's', 'w', 'e', 'l', 'l', 'f', 'o', 'u', 'g', 'h', 't', ';', 'w', 'e', 'a', 'r', 'e', 'c', 'o', 'm', 'e', 'o', 'f', 'f', 'l', 'i', 'k', 'e', 'r', 'o', 'm', 'a', 'n', 's', 'n', 'e', 'i', 't', 'h', 'e', 'r', 'f', 'o', 'o', 'l', 'i', 's', 'h', 'i', 'n', 'o', 'u', 'r', 's', 't', 'a', 'n', 'd', 's', 'n', 'o', 'r', 'c', 'o', 'w', 'a', 'r', 'd', 'l', 'y', 'i', 'n', 'r', 'e', 't', 'i', 'r', 'e', 'b', 'e', 'l', 'i', 'e', 'v', 'e', 'm', 'e', 's', 'i', 'r', 's', 'w', 'e', 's', 'h', 'a', 'l', 'l', 'b', 'e', 'c', 'h', 'a', 'r', 'g', 'e', 'd', 'a', 'g', 'a', 'i', 'n', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['e', 'r', 'e', 'u', 'n', 'h', 'e', 'r', 'o', 'n', 'i', 'n', 't', 'n', 'e', 's', 'n', 'r', 'n', 'o', 'u', 'a', 'e', 'e', 't', 'e', 'r', 'o', 'o', 'd', 's', 't', 'n', 'o', 'n', 'd', 'u', 'e', 't', 'h', 's', 't', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'r', 'l', 'd', 'n', 's', 'n', 'e', 't', 'o', 'r', 'e', 'h', 's', 't', 'n', 'a', 'n', 'i', 'n', 'o', 't', 'h', 'e', 'r', 'a', 'o', 'u', 'h', 'o', 'r', 'e', 'h', 'e', 'r', 'o', 'o', 't', 's', 'e', 't', 'a', 'l', 'e', 'e', 'a', 'r', 'n', 'd', 'i', 'e', 'u', 'h', 'e', 'r', 'l', 't', 'e', 'n']\n",
      "Test accuracy is 5.0%.\n"
     ]
    }
   ],
   "source": [
    "best_model = LSTM(36)\n",
    "best_model = torch.load('best_model.pt')\n",
    "# For each sentence in validation set\n",
    "for inputs, targets in test_set:\n",
    "    \n",
    "    # One-hot encode input and target sequence\n",
    "    inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "    targets_idx = [char_to_idx[char] for char in targets]\n",
    "    \n",
    "    # One-hot encode input and target sequence\n",
    "    inputs_one_hot = one_hot_encode_sequence(test_input_sequence, vocab_size, char_to_idx)\n",
    "    targets_idx = [char_to_idx[char] for char in test_target_sequence]\n",
    "    \n",
    "    \n",
    "    # Convert input to tensor\n",
    "    inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "    inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "    \n",
    "    # Convert target to tensor\n",
    "    targets_idx = torch.LongTensor(targets_idx)\n",
    "    \n",
    "    # Forward pass\n",
    "    # TODO:\n",
    "    outputs = best_model.forward(inputs_one_hot).data.numpy()\n",
    "\n",
    "    print('\\nInput sequence:')\n",
    "    print(inputs)\n",
    "    \n",
    "    print('\\nTarget sequence:')\n",
    "    print(targets)\n",
    "    \n",
    "    preds = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "    print('\\nPredicted sequence:')\n",
    "    print(preds)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for target, pred in zip(targets, preds):\n",
    "        accuracy += target == pred\n",
    "    accuracy /= len(targets)/100\n",
    "    \n",
    "    print(f\"Test accuracy is {np.round(accuracy)}%.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments (extracurricular)\n",
    "Now you've seen how RNNs work and implemented a very simple causal language model. If you want to improve it, besides grabbing more data and building bigger models, you could also improve on character-based tokens to n_grams or subwords; make your word embeddings (hidden state) contextual, upgrade your model to a transformer or a state space model, and suddenly you've designed GPT, BERT, T5, LLaMA, or any other \"modern\" large language model. Though RNNs aren't the most popular due to the dominance of the transformer, models such as state space models, which exist inbetween RNNs and transformers, show that much innovation is still possible by merging these two frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
